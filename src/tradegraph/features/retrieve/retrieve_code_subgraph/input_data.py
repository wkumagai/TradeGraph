retrieve_code_subgraph_input_data = {
    "research_study_list": [
        {
            "title": "Detecting, Explaining, and Mitigating Memorization in Diffusion Models",
            "full_text": "Published as a conference paper at ICLR 2024DETECTING , E XPLAINING , AND MITIGATING MEMO -RIZATION IN DIFFUSION MODELSYuxin Wen1\u2217, Yuchen Liu2\u2217, Chen Chen3, Lingjuan Lyu3\u20201University of Maryland, 2Zhejiang University, 3Sony AIywen@umd.edu, yuchen.liu.a@zju.edu.cn{ChenA.Chen,Lingjuan.Lv}@sony.comABSTRACTRecent breakthroughs in diffusion models have exhibited exceptional image-generation capabilities. However, studies show that some outputs are merelyreplications of training data. Such replications present potential legal chal-lenges for model owners, especially when the generated content contains pro-prietary information. In this work, we introduce a straightforward yet effec-tive method for detecting memorized prompts by inspecting the magnitude oftext-conditional predictions. Our proposed method seamlessly integrates with-out disrupting sampling algorithms, and delivers high accuracy even at the firstgeneration step, with a single generation per prompt. Building on our detec-tion strategy, we unveil an explainable approach that shows the contribution ofindividual words or tokens to memorization. This offers an interactive mediumfor users to adjust their prompts. Moreover, we propose two strategies i.e., tomitigate memorization by leveraging the magnitude of text-conditional predic-tions, either through minimization during inference or filtering during training.These proposed strategies effectively counteract memorization while maintain-ing high-generation quality. Code is available at https://github.com/YuxinWenRick/diffusion_memorization.1 I NTRODUCTIONRecent advancements in diffusion models have revolutionized image generation, with modern text-to-image diffusion models, such as Stable Diffusion and Midjourney, demonstrating unprecedentedcapabilities in generating diverse, stylistically varied images. However, a growing body of research(Somepalli et al., 2022; Carlini et al., 2023; Somepalli et al., 2023b) reveals a concerning trend:some of these \u201cnovel\u201d creations are, in fact, near-exact reproductions of images from their trainingdatasets, as depicted in the top row of Fig. 1. Some of the creations appear to borrow elementsfrom these training images, as illustrated in the second row of Fig. 1. This issue of unintendedmemorization poses a serious concern for model owners and users, especially when the training datacontains sensitive or copyrighted material. A poignant real-life example of this isMidjourney, whichfelt obliged to ban prompts with the substring \u201cAfghan\u201d to avoid generating images reminiscent ofthe renowned copyrighted photograph of the Afghan girl. Yet, as Wen et al. (2023a) note, merelybanning the term \u201cAfghan\u201d does not prevent the model from recreating those images. In light ofthese issues, the development of techniques to detect and address such inadvertent memorizationshas become crucial.To address this, we first introduce a novel method for detecting memorized prompts. We\u2019ve observedthat for such prompts, the text condition consistently guides the generation towards the memorizedsolution, regardless of the initializations. This phenomenon suggests significant text guidance duringthe denoising process. As a result, our detection method prioritizes the magnitude of text-conditionalpredictions as its cardinal metric. Distinctively, memorized prompts tend to exhibit larger magni-tudes than non-memorized ones, as showcased in Fig. 1. Unlike previous methods that query largetraining datasets with generated images (Somepalli et al., 2022) or assess density across multiple\u2217Work done during an internship at Sony AI.\u2020Corresponding author.1arXiv:2407.21720v1  [cs.CV]  31 Jul 2024Published as a conference paper at ICLR 2024Training Image Generated Image Magnitude of Text-ConditionalNoise Prediction02505007501000Time-step05101520Metric0123\u201cLiving in the Light with Ann Graham Lotz\u201d02505007501000Time-step05101520Metric\u201cPlattville Green Area Rug by Andover Mills\u201d02505007501000Time-step05101520Metric\u201cBeautiful Bernese mountain dog (Berner Sennenhund) lies on snow in park\u201dFigure 1: Memorization vs. non-memorization generation. We display the magnitude of text-conditional noise prediction at each time-step, as described in Section 3.3, for all four generationsdistinctly (with 4 different random seeds) for each prompt. As illustrated in the first two rows, themetric typically indicates a higher value when memorization occurs. On the other hand, the normalgenerations, represented in the third row, consistently exhibit significantly lower metric values.generations (Carlini et al., 2023), our strategy offers precise detection without adding any extra workto the existing generation framework and even without requiring multiple generations. In terms ofefficacy, our method achieves an AUC of0.960 and a TPR@1%FPR of 0.760 in under 2 seconds. Incontrast, the baseline method (Carlini et al., 2023) demands over39 seconds, even though it registersan AUC of 0.934 and a TPR @1%FPR of 0.523. Such efficiency empowers model owners to haltgenerations early and initiate corrective measures promptly upon detecting a memorized prompt.Building on our discoveries, we devise a strategy to highlight the influence of each token in driv-ing memorization, aiming to pinpoint the specific trigger tokens responsible for it. Following theintuition that removing these trigger tokens should neutralize memorization, we anticipate a cor-responding reduction in the magnitudes of text-conditional predictions. Thus, by evaluating thegradient change for every token when minimizing text-conditional prediction magnitudes, we dis-cern the relative significance of every token to memorization. Armed with this tool, model ownerscan provide constructive feedback to users, guiding them to identify, modify, or omit these pivotaltrigger tokens, effectively reducing the propensity for memorization. In contrast to earlier work like(Somepalli et al., 2023b), where trigger tokens were discerned manually from training data or byexperimenting with various token combinations, our approach stands out for its automated natureand computational efficiency.Lastly, we introduce mitigation strategies to address memorization concerns. Catering to both in-ference and training phases, we present model owners with a choice of two distinct tactics. Forinference, we suggest using a perturbed prompt embedding, achieved by minimizing the magni-tude of text-conditional predictions. During training, potential memorized image-text pairs can bescreened out based on the magnitude of text-conditional predictions. Our straightforward approaches2Published as a conference paper at ICLR 2024ensure a more consistent alignment between prompts and generations, and they effectively reducethe memorization effect when benchmarked against baseline mitigation strategies.2 R ELATED WORKMembership Inference. The membership inference attack (Shokri et al., 2017) aims to determineif a particular data point was used in the training set of a model. Traditional studies on membershipinference (Shokri et al., 2017; Yeom et al., 2018; Carlini et al., 2022; Wen et al., 2022) have predom-inantly focused on classifiers. An attacker can utilize losses or confidence scores as a metric. This isbecause data points from the training set typically exhibit lower losses or higher confidence scoresthan the unseen data points during inference due to overfitting. In a parallel development, recentworks (Matsumoto et al., 2023; Duan et al., 2023; Wang et al., 2024; 2023) have extended member-ship inference to diffusion models. These methodologies involve introducing noise to a target imageand subsequently verifying if the predicted noise aligns closely with the induced noise.Training Data Extraction. Somepalli et al. (2022) demonstrate that diffusion models memorizea subset of their training data, often producing the training image verbatim. Building on this fact,Carlini et al. (2023) introduce a black-box data extraction attack designed for diffusion models.This approach involves generating a multitude of images and subsequently applying a membershipinference attack to assess generation density. Notably, they observe that memorized prompts tend toproduce nearly identical images across different seeds, leading to high density. This strategy bearsresemblance to the pipeline used by Carlini et al. (2021), who successfully extract training data fromlarge language models with over a billion parameters. Additionally, they discover that larger modelsare more susceptible to data extraction attacks compared to their smaller counterparts.Diffusion Memorization Mitigation. Recent research by Daras et al. (2023) presents a method fortraining diffusion models using corrupted data. In their study, they demonstrate that their proposedtraining algorithm aids in preventing the model from overfitting to the training data. Their approachinvolves introducing additional corruption prior to the noising step and subsequently calculating theloss on the original input image. In a separate study, Somepalli et al. (2023b) delve into variousmitigation strategies, with a focus on altering the text conditions. As a notable example, by insertingrandom tokens into the prompt or integrating random perturbations into the prompt embedding, theyalleviate the memorization concern while preserving a high-quality generation output.3 D ETECT MEMORIZATION EFFICIENTLY3.1 P RELIMINARYWe begin by defining the essential notation associated with diffusion models (Ho et al., 2020; Song& Ermon, 2020; Dhariwal & Nichol, 2021). For a data pointx0 drawn from the real data distributionq(x0), a forward diffusion process comprises a fixed Markov chain spanning T steps, where eachstep introduces a predetermined amount of Gaussian noise. Specifically:q(xt|xt\u22121) = N(xt;p1 \u2212 \u03b2txt, \u03b2tI), for t \u2208 {1, ..., T}, (1)where \u03b2t \u2208 (0, 1) is the scheduled variance at step t. The closed-form for this sampling isxt = \u221a\u00af\u03b1tx0 +\u221a1 \u2212 \u00af\u03b1t\u03f5, (2)where, \u00af\u03b1t = Qti=1(1 \u2212 \u03b2t).In the reverse diffusion process, a Gaussian vector xT \u223c N(0, 1) is denoised to map to an imagex0 \u2208 q(x). At each denoising step, a trained noise-predictor \u03f5\u03b8 anticipates the noise \u03f5\u03b8(xt) that wasadded to x0. Based on Eq. (2), the estimation of x0 can be formulated as:\u02c6xt0 = xt \u2212 \u221a1 \u2212 \u00af\u03b1t\u03f5\u03b8(xt)\u221a\u00af\u03b1t. (3)Then, we can predict xt\u22121 as:xt\u22121 = \u221a\u00af\u03b1t\u22121 \u02c6xt0 +p1 \u2212 \u00af\u03b1t\u22121\u03f5\u03b8(xt). (4)3Published as a conference paper at ICLR 202402004006008001000Time-step2 1202122232425MagnitudeMemorizationNon-Memorizaton5/95th Percentile(a) Lineplot over generation steps100 101Magnitude0.00.20.40.60.81.01.2DensityMemorization, AUC=0.993, TPR@1%FPR=0.978LAIONCOCOLexica.artRandom (b) Histogram over different datasetsFigure 2: Statistics of the magnitude of text-conditional noise predictions.Text-conditional diffusion models, such as Stable Diffusion (Rombach et al., 2022), employclassifier-free diffusion guidance (Rombach et al., 2022) to steer the sampling process. Given atext prompt p, its embedding ep = f(p) is computed using a pre-trained CLIP text encoder f(\u00b7)(Radford et al., 2021; Cherti et al., 2023). In the reverse process, the conditional sampling adheresto Eq. (3) and Eq. (4), but the predicted noise \u03f5\u03b8(xt) is changed to:\u03f5\u03b8(xt, e\u2205) + s(\u03f5\u03b8(xt, ep) \u2212 \u03f5\u03b8(xt, e\u2205)| {z }text-conditional noise prediction),where, e\u2205 represents the prompt embedding of an empty string, and s determines the guidancestrength, controlling the alignment of the generation to the prompt. We refer to the term\u03f5\u03b8(xt, ep)\u2212\u03f5\u03b8(xt, e\u2205) as the text-conditional noise prediction for future reference.3.2 M OTIVATIONWhen provided with the same text prompt but different initializations, diffusion models can generatea diverse set of images. Conversely, when given different text prompts but the same initialization,the resulting images often display semantic similarities. These similarities include analogous layoutsand color themes, as demonstrated in Appendix Fig. 6. Such a phenomenon might arise when thefinal generation remains closely tied to its initialization, and the textual guidance is not particularlydominant. This observation is consistent with findings from Wen et al. (2023b), suggesting that onecan trace the origin to the initial seed even without knowing the text condition.Interestingly, when it comes to memorized prompts, the initialization appears to be irrelevant. Thegenerated images consistently converge to a specific memorized visualization. This behavior impliesthat the model might be overfitting to both the prompt and a certain denoising trajectory, which leadsto the memorized image. Consequently, the final image deviates substantially from its initial state.These insights provide a foundation for a straightforward detection strategy: scrutinizing the magni-tude of text-conditional noise predictions. A smaller magnitude signals that the final image is closelyaligned with its initialization, hinting that it is likely not a memorized image. On the other hand,a larger magnitude could indicate potential memorization. The correlation between magnitude andmemorization is depicted in Fig. 2(a).3.3 A N EFFECTIVE DETECTION METHODFollowing the intuition above, we introduce a straightforward yet effective detection method cen-tered on the magnitude of text-conditional noise predictions. For a prompt embedding p and asampling step of T, the detection metric is defined asd = 1TTXt=1\u2225\u03f5\u03b8(xt, ep) \u2212 \u03f5\u03b8(xt, e\u2205)\u22252.4Published as a conference paper at ICLR 2024Table 1: Memorization detection results with AUC, TPR @1%FPR, and the running time of themethod in seconds. In this table, \u201cn\u201d represents the number of generations per prompt.Method 1st Step First 10 Steps Last StepAUC\u2191 / TPR@1%FPR\u2191 / Time in Seconds\u2193Density\u21132 , n=4 0.520 / 0.012 / 0.810 0 .652 / 0.225 / 5.314 0 .659 / 0.288 / 9.904Density\u21132 , n=16 0.506 / 0.000 / 3.570 0 .656 / 0.175 / 24.78 0 .676 / 0.271 / 40.66Density\u21132 , n=32 0.510 / 0.000 / 8.092 0 .664 / 0.175 / 59.43 0 .681 / 0.266 / 81.44DensitySSCD, n=4 0.537 / 0.019 / 0.809 0 .405 / 0.005 / 5.421 0 .878 / 0.525 / 9.892DensitySSCD, n=16 0.515 / 0.000 / 3.186 0 .375 / 0.000 / 21.02 0 .934 / 0.523 / 39.55DensitySSCD, n=32 0.506 / 0.000 / 6.341 0 .370 / 0.000 / 42.12 0 .940 / 0.530 / 79.47Ours, n=1 0.960 / 0.760 / 0.199 0.989 / 0.944 / 1.866 0.989 / 0.934 / 9.584Ours, n=4 0.990 / 0.912 / 0.794 0 .998 / 0.982 / 7.471 0 .996 / 0.978 / 37.27Ours, n=32 0.996 / 0.954 / 1.606 0.999 / 0.988 / 14.96 0.998 / 0.986 / 74.75Memorization is then identified if the detection metric falls beneath a tunable threshold \u03b3.In practice, we also find that even the detection metric of a single generation can provide a strongsignal of memorization. Consequently, our method remains effective and reliable with the number ofgenerations restricted to 1. In contrast, earlier studies, such as those examining generation densityover a large number of generations (Carlini et al., 2023), require the simultaneous generation ofmultiple images, with some cases necessitating over a hundred generations. This might imposean extra computational burden on the service provider by generating more images than the userrequested. Moreover, another method presented in (Somepalli et al., 2023a) identifies memorizedprompts by directly comparing the generated images with the original training data. Unlike thismethod, our approach allows a third party to use the detection method without needing access to thelarge training dataset, thereby protecting training data privacy.Another distinct advantage of our approach is its adaptability in calculating the detection metric.Strong detection does not mandate collecting the metric from all sampling steps. Based on ourempirical findings, even when the metric is collated solely from the first step, reliable detectionremains attainable. This efficiency enables model owners to identify memorized prompts promptly.By stopping generation early, they can then opt for post-processing, like declining the generatedoutput or reinitializing the generation with corrective strategies in place.3.4 E XPERIMENTSExperimental Setup. To evaluate our detection method, we use500 memorized prompts identifiedin Webster (2023) for Stable Diffusion v1 (Rombach et al., 2022), where the SSCD similarity score(Pizzi et al., 2022) between the memorized and the generated images exceeds 0.7. The memorizedprompts gathered in Webster (2023) include three types of memorization: 1) matching verbatim:where the images generated from the memorized prompt are an exact pixel-by-pixel match with theoriginal paired training image; 2) retrieval verbatim: the generated images perfectly align with sometraining images, albeit paired with different prompts; 3) template verbatim: generated images beara partial resemblance to the training image, though variations in colors or styles might be observed.Additionally, we use another 2, 000 prompts, evenly distributed from sources LAION (Schuhmannet al., 2022), COCO (Lin et al., 2014), Lexica.art (Santana, 2022), and randomly generated strings.For this set of prompts, we assume they are not memorized by the model. All generations employDDIM (Song et al., 2020) with 50 inference steps.In our comparison, we use the detection method from Carlini et al. (2023) as a baseline. This methoddetermines memorization by analyzing generation density, computed using the pairwise \u21132 distanceIn our comparison, we use the detection method from Carlini et al. (2023) as a baseline. This methoddetermines memorization by analyzing generation density, computed using the pairwise \u21132 distancebetween non-overlapping tiles. While Carlini et al. (2023) utilizes the \u21132 distance in pixel space, weintroduce an additional baseline that calculates the distance in the SSCD feature space (Pizzi et al.,2022). This adjustment is inspired by Somepalli et al. (2022), who underscore the effectiveness of5Published as a conference paper at ICLR 2024SSCD. As a deep learning-informed distance metric, SSCD offers enhanced resilience to particularaugmentations, like color shift \u2014 a critical advantage when the training image is only partiallymemorized.We use the area under the curve (AUC) of the receiver operating characteristic (ROC) curve andthe True Positive Rate at the False Positive Rate of 1% (TPR@1%FPR) as metrics. Meanwhile, wereport the running time in seconds with a batch size of 4 on a single NVIDIA RTX A6000.Results. In Fig. 2(b), we display a density plot comparing the detection metrics for the memorizedprompts against the non-memorized ones, calculated over 50 steps with 4 generations per prompt.The distribution of memorized prompts is bimodal. This dichotomy stems from the fact that the tem-plate verbatim scenario often exhibits a slightly smaller metric than the matching verbatim scenario,given that the memorization occurs only partially.In Table 1, we highlight the balance between the precision and efficiency of our proposed method.Our method is able to achieve very strong detection performance. When generating 32 imagesand using the metrics from the first 10 steps, our method is able to achieve an AUC of 0.999 andTPR@1%FPR of 0.988. Remarkably, even when operating with a single generation, our methodcan achieve TPR@1%FPR of 0.760 from the very first step within merely 0.2 seconds. This featureprovides a significant advantage in terms of time and computational resource savings, allowingmodel operators the flexibility to terminate generation early if necessary. In contrast, the baselinemethods show noticeably reduced detection capability. In particular, the baseline methods can onlyachieve relatively high detection accuracy when generating more than 16 images and relying onthe image generation from the final step, where it requires at least 40 seconds. Yet, in real-worldapplications, service providers likeMidjourney or DALL-E 2(Ramesh et al., 2022) typically generatea mere 4 images concurrently for each prompt.Interestingly, our method surpasses the baselines in speed even when using the metric with equiva-lent generations and steps. This superiority emerges since our method doesn\u2019t need to decode latentnoise into image space and perform subsequent calculations.4 M ITIGATE MEMORIZATION4.1 A S TRAIGHTFORWARD METHOD TO DETECT TRIGGER TOKENSAs observed by Somepalli et al. (2022), certain words or tokens in memorized prompts play a sig-nificant influence on the generation process. Even when only these specific \u201ctrigger tokens\u201d arepresent in the prompt, the memorization effect remains evident. One potential approach to identifythese trigger tokens involves probing with various n-gram combinations to discern which combina-tions induce memorization. However, this heuristic becomes notably inefficient, particularly whenthe prompt contains a vast number of tokens. Our earlier observations offer a more streamlinedmethod for discerning the significance of each token in relation to memorization: by checking themagnitude of the change applied to each token while minimizing the magnitude of text-conditionalnoise prediction. A token undergoing substantial change suggests its crucial role in steering theprediction; conversely, a token with minimal change is less important.Given a prompt embedding e of prompt p with N tokens, we form the objective of the minimizationproblem as:L(xt, e) = \u2225\u03f5\u03b8(xt, e) \u2212 \u03f5\u03b8(xt, e\u2205)\u22252. (5)We then determine the significance score for each token at position i \u2208 [0, N\u2212 1] as:SSei = 1TTXt=1\u2225\u2207ei L(xt, e)\u22252.In Fig. 3, we display generations with top-2 significant tokens highlighted. The green arrow in thefigure emphasizes that altering these significant tokens can substantially diminish the memorizationeffect. Some trigger tokens, including symbols or seemingly trivial words, are challenging to iden-effect. Some trigger tokens, including symbols or seemingly trivial words, are challenging to iden-tify manually. Consequently, this insight offers model owners a practical tool: advising users torephrase or exclude the trigger tokens before initiating another generation. In contrast, as indicated6Published as a conference paper at ICLR 2024Training Image Original Generation After User Modification1. Rewrite goats to sheep2. Delete .\u201cA pair of mountain goats stand proudly, high in the Rocky Mountains.\u201d1. Rewrite Wave to Swell2. Rewrite Kanagawa to Tokyo\u201cA painting of the Great Wave off Kanagawa by Katsushika Hokusai\u201dUser Modification1. Delete \u201cproudly, high\u201d2. Rewrite \u201cthe Rocky Mountains\u201d to \u201cBlue Ridge Mountains\u201d1. Delete \u201cby Katsushika Hokusai\u201d2. Delete \u201cGreat\u201dFigure 3: By modifying the trigger tokens, memorization can be effectively mitigated. The signifi-cance score for each token is illustrated in a histogram in Appendix Fig. 7. The two most significanttokens are highlighted in red and blue. A green arrow indicates modifications made to the top-2tokens, while a white arrow represents changes to less significant tokens.by the white arrow, alterations to the less significant tokens fail to effectively counter the memo-rization effect, even when extensive changes are made. Even by renaming the play or removing theartist\u2019s name, memorization remains evident.4.2 A N EFFECTIVE INFERENCE -TIME MITIGATION METHODA direct approach to mitigation without any supervision is to adjust the prompt embedding by min-imizing Eq. (5). Optimizing over all time steps is computationally intensive. However, we observethat minimizing the loss at the initial time step indirectly results in smaller magnitudes in subse-quent time steps, effectively mitigating memorization. Thus, a perturbed prompt embedding, e\u2217, isobtained at t = 0 by minimizing Eq. (5). We also apply early stopping once the loss reaches a targetvalue, ltarget, to keep the embedding close to the original meaning.4.3 A N EFFECTIVE TRAINING -TIME MITIGATION METHODDuring the training of diffusion models, memorization often arises with duplicate data points dueto overfitting (Carlini et al., 2023; Somepalli et al., 2023b). Building on our earlier observation,if the model overfits or closely memorizes a data point, the magnitude of the text-conditional noise7Published as a conference paper at ICLR 20240.25 0.26 0.27 0.28 0.29 0.30CLIP Score0.10.20.30.40.5Similarity ScoreBefore Fine-TuningW/o MitigationRandom T oken AdditionOurs(a) Inference-time mitigation0.294 0.296 0.298CLIP Score0.10.20.30.40.5Similarity Score (b) Training-time mitigationFigure 4: Mitigation results. A lower similarity score suggests reduced memorization, whereas ahigher clip score denotes a better alignment between the generation and the prompt.prediction might exceed typical values. Therefore, a straightforward mitigation method is to excludethe sample from the current mini-batch if this magnitude surpasses a predetermined threshold, \u03c4,thereby not computing the loss on that sample. Given that the model has previously seen the sampleduring training, this exclusion is unlikely to significantly impact model performance.However, this method introduces an additional computational cost during training. To compute thetext-conditional noise prediction, an extra forward pass for \u03f5\u03b8(xt, e\u2205) is needed. During typicaltraining, only \u03f5\u03b8(xt, ep) is computed. Empirically, this results in an approximately 10% increase intraining time.4.4 E XPERIMENTSExperimental Setup. To evaluate the effectiveness of mitigation strategies, we adopt a setup sim-ilar to that in Somepalli et al. (2023b). Specifically, we fine-tune the Stable Diffusion model using200 LAION data points, each duplicated 200 times, to serve as memorized prompts. In addition,we introduce 120, 000 distinct LAION data points to ensure that the model retains its capacity forgeneralization.For performance metrics, we compute the SSCD similarity score (Pizzi et al., 2022; Somepalli et al.,2023b) to gauge the degree of memorization by comparing the generation to the original image.Additionally, the CLIP score (Radford et al., 2021) is used to quantify the alignment between thegeneration and its corresponding prompt. Our experiments encompass 5 distinct fine-tuned models,each embedded with different memorized prompts, and the results are averages over 5 runs withdifferent random seeds.In our evaluation of the proposed method, we test 5 distinct target losses ltarget, ranging from 1 to5, for inference-time mitigation. We use Adam optimizer (Kingma & Ba, 2014) with a learningrate of 0.05 and at most 10 steps. Simultaneously, we investigate 5 different thresholds \u03c4, spanningfrom 2 to 6, for training-time mitigation. For comparison, we use the most effective method from(Somepalli et al., 2023b), random token addition (RTA), as the baseline, which inserts 1, 2, 4, 6, or8 random tokens to the prompt.Results. In Fig. 4(a), we present the results of our inference-time mitigation, while Fig. 4(b) de-tails the outcomes for training-time mitigation. Our proposed techniques successfully mitigate thememorization effect and, importantly, offer a more favorable CLIP score trade-off compared to RTA.Higher target losses ltarget or thresholds \u03c4 tend to enhance the model\u2019s alignment with the promptbut can result in a less pronounced mitigation effect. In practice, model owners can select the op-timal hyperparameter based on their desired balance between mitigation efficacy and generationalignment.8Published as a conference paper at ICLR 2024Training Image W/o Mitig. Ours(Inference-time)RTA(Inference-time)Ours(Training-time)RTA(Training-time)\u201cPortrait Of A Woman\u201d\u201cTHE STANDARD, LONDON - INTERIOR: The central London hotel, opposite St Pancras Station, features 'colourful interiors to contrast the greyness of London'\u201d\u201ctable\u201dFigure 5: Mitigation results with different mitigation strategies during inference and training phase.In Fig. 5, we display a selection of qualitative results, setting ltarget = 3 and \u03c4 = 4 for our method,while adding 4 random tokens as a baseline strategy. Our proposed strategy is effective in mitigat-ing the memorization effect while also ensuring that the generated content aligns closely with theprompt. In contrast, the baseline method often struggles to counteract memorization and occasion-ally produces images with undesirable additions.5 L IMITATIONS AND FUTURE WORKOur detection strategy employs a tunable threshold to detect memorized prompts. This requiresmodel owners to first compute metrics over non-memorized prompts and then select an empiricalthreshold based on a predetermined false positive rate. However, the outcomes generated by thisdetection approach lack interpretability. In the future, the development of a method producing in-terpretable p-values could significantly assist model owners by providing a confidence score thatquantifies the likelihood of memorization, thereby augmenting the transparency and trustworthinessof the detection process.Our proposed mitigation strategies effectively tackle the memorization problem, albeit with minorcomputational overheads for model owners. The inference-time mitigation approach requires addi-tional GPU memory due to the optimization process, and in practice, it takes at most6 seconds withour setup. On the other hand, the training-time mitigation extends the training period by roughly10%. For context, while naive fine-tuning in our tests takes approximately 9 hours, our mitigationstrategy extends this to about 10 hours. However, we argue that these added costs are reasonablegiven the significance of protecting training data privacy and intellectual property. Furthermore,when combined with our detection method, model owners need only deploy the inference-time mit-igation when a memorized prompt is detected, thereby minimizing its use.6 C ONCLUSIONIn this paper, we introduced a new approach to detect memorization in diffusion models by lever-aging the magnitude of text-conditional noise predictions. Remarkably, our approach attains highprecision even when using a limited number of generations per prompt and a limited number ofsampling steps. Furthermore, we provide an explanatory tool to indicate the significance score ofindividual tokens in relation to memorization. To conclude, our paper presents both inference-timeand training-time mitigation strategies. These not only effectively address the memorization concernbut also maintain the superior generative performance of the model.9Published as a conference paper at ICLR 20247 R EPRODUCIBILITY STATEMENTWe have detailed all essential hyperparameters used in our experiment setup in the main body. Ourexperiments were conducted using widely available computing resources and open-source software.All referenced models and datasets in this paper are publicly accessible. Furthermore, we haveincluded the code necessary to reproduce our results in the supplementary material.8 D ISCLAIMERThis research required the use of some images from the LAION 5B dataset for analysis purposes.After our use and deletion of such images, it was discovered that LAION 5B contained exploitativeimages of children. While we have no reason to believe that such harmful images were present in oursubset, we acknowledge child safety is a serious issue and believe it is important to be transparentabout the dataset\u2019s use and its associated ethical considerations.REFERENCESNicholas Carlini, Florian Tram `er, Eric Wallace, Matthew Jagielski, Ariel Herbert-V oss, KatherineLee, Adam Roberts, Tom Brown, Dawn Song, \u00b4Ulfar Erlingsson, Alina Oprea, and Colin Raf-fel. Extracting training data from large language models. In 30th USENIX Security Sympo-sium (USENIX Security 21), pp. 2633\u20132650. USENIX Association, August 2021. ISBN 978-1-939133-24-3. URL https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting.Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Mem-bership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy(SP), pp. 1897\u20131914. IEEE, 2022.Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tram `er,Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting Training Data from Diffusion Models.arxiv:2301.13188[cs], January 2023. doi: 10.48550/arXiv.2301.13188. URL http://arxiv.org/abs/2301.13188.Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gor-don, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws forcontrastive language-image learning. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 2818\u20132829, 2023.Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alexandros G Dimakis, and AdamKlivans. Ambient diffusion: Learning clean distributions from corrupted data. arXiv preprintarXiv:2305.19256, 2023.Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis.arxiv:2105.05233[cs, stat], June 2021. doi: 10.48550/arXiv.2105.05233. URL http://arxiv.org/abs/2105.05233.Jinhao Duan, Fei Kong, Shiqi Wang, Xiaoshuang Shi, and Kaidi Xu. Are diffusion models vulnera-ble to membership inference attacks? arXiv preprint arXiv:2302.01316, 2023.Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Ad-vances in Neural Information Processing Systems, volume 33, pp. 6840\u20136851. Curran Asso-ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprintarXiv:1412.6980, 2014.Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, PiotrDoll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ComputerVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,Proceedings, Part V 13, pp. 740\u2013755. Springer, 2014.10Published as a conference paper at ICLR 2024Tomoya Matsumoto, Takayuki Miura, and Naoto Yanai. Membership inference attacks againstdiffusion models. arXiv preprint arXiv:2302.03262, 2023.Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised descriptor for image copy detection. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 14532\u201314542, 2022.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and IlyaSutskever. Learning transferable visual models from natural language supervision. In Interna-tional Conference on Machine Learning, 2021.Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj \u00a8orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-ence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.Gustavo Santana. Gustavosta/Stable-Diffusion-Prompts \u00b7 Datasets at Hugging Face, De-cember 2022. URL https://huggingface.co/datasets/Gustavosta/Stable-Diffusion-Prompts.Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, MehdiCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: Anopen large-scale dataset for training next generation image-text models. Advances in NeuralInformation Processing Systems, 35:25278\u201325294, 2022.Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-tacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP),pp. 3\u201318. IEEE, 2017.Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein.Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models.arxiv:2212.03860[cs], December 2022. doi: 10.48550/arXiv.2212.03860. URL http://arxiv.org/abs/2212.03860.Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusionart or digital forgery? investigating data replication in diffusion models. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6048\u20136058,June 2023a.Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Under-standing and mitigating copying in diffusion models. arXiv preprint arXiv:2305.20086, 2023b.Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXivpreprint arXiv:2010.02502, 2020.Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models.arXiv:2006.09011 [cs, stat], June 2020. URL http://arxiv.org/abs/2006.09011.Zhenting Wang, Chen Chen, Lingjuan Lyu, Dimitris N Metaxas, and Shiqing Ma. Diagnosis: De-tecting unauthorized data usages in text-to-image diffusion models. In The Twelfth InternationalConference on Learning Representations, 2023.Zhenting Wang, Chen Chen, Yi Zeng, Lingjuan Lyu, and Shiqing Ma. Where did i come from?origin attribution of ai-generated images. Advances in Neural Information Processing Systems,36, 2024.Ryan Webster. A reproducible extraction of training images from diffusion models. arXiv preprintarXiv:2305.08694, 2023.11Published as a conference paper at ICLR 2024Yuxin Wen, Arpit Bansal, Hamid Kazemi, Eitan Borgnia, Micah Goldblum, Jonas Geiping, andTom Goldstein. Canary in a coalmine: Better membership inference with ensembled adversarialqueries. arXiv preprint arXiv:2210.10750, 2022.Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein.Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.arXiv preprint arXiv:2302.03668, 2023a.Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fin-gerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030,2023b.Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learn-ing: Analyzing the connection to overfitting. In 2018 IEEE 31st computer security foundationssymposium (CSF), pp. 268\u2013282. IEEE, 2018.12Published as a conference paper at ICLR 2024A A PPENDIXSeed 0Seed 3Seed 2Seed 1Normal Prompt Memorized PromptFigure 6: Different seeds vs. different prompts.13Published as a conference paper at ICLR 2024Training Image Original Generation After User Modification1. Rewrite goats to sheep2. Delete .\u201cA pair of mountain goats stand proudly, high in the Rocky Mountains.\u201d1. Rewrite Wave to Swell2. Rewrite Kanagawa to Tokyo\u201cA painting of the Great Wave off Kanagawa by Katsushika Hokusai\u201dUser Modification1. Delete \u201cproudly, high\u201d2. Rewrite \u201cthe Rocky Mountains\u201d to \u201cBlue Ridge Mountains\u201d1. Delete \u201cby Katsushika Hokusai\u201d2. Delete \u201cGreat\u201dFigure 7: By modifying the trigger tokens, memorization can be effectively mitigated. The signifi-cance score for each token is illustrated in a histogram in Appendix Fig. 7. The two most significanttokens are highlighted in red and blue. A green arrow indicates modifications made to the top-2tokens, while a white arrow represents changes to less significant tokens.14Published as a conference paper at ICLR 2024Training Image Generated Image Magnitude of Text-ConditionalNoise Prediction02505007501000Time-step05101520Metric0123\u201cPrince Reunites With Warner Brothers, Plans New Album\u201d02505007501000Time-step05101520Metric\u201cThe No Limits Business Woman Podcast\u201d02505007501000Time-step05101520Metric\u201cThe Happy Scientist\u201d02505007501000Time-step05101520Metric\u201cNetflix Strikes Deal with AT&T for Faster Streaming\u201d02505007501000Time-step05101520Metric\u201cPencil pleat curtains in collection Avinon, fabric: 129-66\u201dFigure 8: Memorization generation. We display the magnitude of text-conditional noise predictionat each time-step, as described in Section 3.3, for all four generations distinctly (with 4 differentrandom seeds) for each prompt. The metric typically indicates a higher value when memorizationoccurs.15Published as a conference paper at ICLR 2024Training Image Generated Image Magnitude of Text-ConditionalNoise Prediction02505007501000Time-step05101520Metric0123\u201cWatercolour painting of Xian fortifications. Sian city wall, China aquarelle illustration.\u201d02505007501000Time-step05101520Metric\u201cpainting #ballet #painting\u201d02505007501000Time-step05101520Metric\u201cNaruto: Ultimate Ninja Storm PS3\u201d02505007501000Time-step05101520Metric\u201cthe marq vietnams ultimate luxury residential destination to be launched\u201d02505007501000Time-step05101520Metric\u201cCroque madame sandwich, delish food\u201dFigure 9: Non-memorization generation. We display the magnitude of text-conditional noise predic-tion at each time-step, as described in Section 3.3, for all four generations distinctly (with 4 differentrandom seeds) for each prompt. The non-memorized generations consistently exhibit significantlylower metric values.16",
            "llm_extracted_info": {
                "main_contributions": "The paper presents a novel approach to detect, explain, and mitigate memorization in diffusion models. The key contributions include a fast and effective detection method based on the magnitude of text-conditional noise predictions, an explainable technique to identify trigger tokens that contribute to memorization, and two mitigation strategies that operate during inference and training to counteract the memorization effect.",
                "methodology": "The approach leverages the observation that memorized prompts yield higher magnitudes of text-conditional noise predictions across diffusion steps. A detection metric is computed as the aggregated norm difference between noise predictions with and without textual guidance. The method also computes a significance score for each token by evaluating the gradient change when minimizing the noise prediction magnitude. Mitigation is achieved by perturbing the prompt embedding at inference and by screening out potential memorized image-text pairs during training.",
                "experimental_setup": "Experiments were conducted on Stable Diffusion v1 using 500 memorized prompts identified from prior work and 2000 non-memorized prompts sourced from datasets like LAION, COCO, and Lexica.art, alongside random strings. The study employed DDIM for sampling with 50 inference steps and benchmarked performance with metrics such as AUC, TPR@1%FPR, SSCD similarity scores, and CLIP scores. Baseline comparisons with existing methods (Carlini et al.) were also included to highlight improvements in speed and accuracy.",
                "limitations": "The detection method relies on a tunable threshold which requires calibration using non-memorized prompts, and the decision process lacks interpretability in terms of providing confidence measures. Additionally, the mitigation approaches introduce extra computational overhead: the inference-time method requires additional GPU memory and optimization steps, and the training-time method increases training duration by about 10%.",
                "future_research_directions": "Future extensions could focus on developing more interpretable detection outputs, such as providing statistical confidence scores or p-values. Research may also explore further reduction of computational overhead in both inference and training-time mitigation, and adaptation of these techniques to other types of generative models and diverse data distributions.",
            },
        }
    ]
}
