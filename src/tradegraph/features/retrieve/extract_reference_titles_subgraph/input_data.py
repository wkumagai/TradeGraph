extract_reference_titles_subgraph_input_data = {
    "research_study_list": [
        {
            "full_text": "Published as a conference paper at ICLR 2024MG-TSD: M ULTI -GRANULARITY TIME SERIES DIF-FUSION MODELS WITH GUIDED LEARNING PROCESSXinyao Fan1∗, Yueying Wu2∗, Chang Xu4†, Yuhao Huang3, Weiqing Liu4, Jiang Bian4University of British Columbia1, Peking University2, Nanjing University3, Microsoft Research4xinyao.fan@stat.ubc.ca, wuyueying@stu.pku.edu.cn,huangyh@smail.nju.edu.cn,{chanx, weiqing.liu, jiang.bian}@microsoft.comABSTRACTRecently, diffusion probabilistic models have attracted attention in generative timeseries forecasting due to their remarkable capacity to generate high-fidelity sam-ples. However, the effective utilization of their strong modeling ability in theprobabilistic time series forecasting task remains an open question, partially dueto the challenge of instability arising from their stochastic nature. To address thischallenge, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves state-of-the-art predictive performance by leverag-ing the inherent granularity levels within the data as given targets at intermedi-ate diffusion steps to guide the learning process of diffusion models. The wayto construct the targets is motivated by the observation that the forward pro-cess of the diffusion model, which sequentially corrupts the data distribution toa standard normal distribution, intuitively aligns with the process of smoothingfine-grained data into a coarse-grained representation, both of which result in agradual loss of fine distribution features. In the study, we derive a novel multi-granularity guidance diffusion loss function and propose a concise implemen-tation method to effectively utilize coarse-grained data across various granular-ity levels. More importantly, our approach does not rely on additional externaldata, making it versatile and applicable across various domains. Extensive exper-iments conducted on real-world datasets demonstrate that our MG-TSD modeloutperforms existing time series prediction methods. Our code is available athttps://github.com/Hundredl/MG-TSD.1 I NTRODUCTIONTime series prediction is a critical task with applications in various domains such as finance fore-casting (Hou et al., 2021; Chen et al., 2018), energy planning (Koprinska et al., 2018; Wu et al.,2021), climate modeling (Wu et al., 2023; 2021), and biological sciences (Luo et al., 2020; Ra-jpurkar et al., 2022). Considering that time series forecasting problems can be effectively addressedas a conditional generation task, many works leverage generative models for predictive purposes.For instance, Salinas et al. (2019) utilizes a low-rank plus diagonal covariance Gaussian copula; Ra-sul et al. (2021) models the predictive distribution using normalizing flows. Recent advancementsin diffusion probabilistic models (Ho et al., 2020) have sparked interest in utilizing them into prob-abilistic time series prediction. For example, Rasul et al. (2020) auto-regressively generates datathrough iterative denoising diffusion models. Tashiro et al. (2021) uses a conditional score-baseddiffusion model explicitly trained for probabilistic time series imputation and prediction. Thesemethods relying on diffusion models have exhibited remarkable predictive capabilities. However,there is still considerable scope for improvement. One challenge that diffusion models face in timeseries forecasting tasks is the instability due to their stochastic nature when compared to determinis-tic models like RNNs and variants like LSTMs (Hochreiter & Schmidhuber, 1997; Lai et al., 2018),GRUs (Ballakur & Arya, 2020; Yamak et al., 2019), and Transformers that rely on self-attentionmechanisms (Vaswani et al., 2017; Zhou et al., 2021; 2022; Wu et al., 2021). More specifically, the∗These authors contributed equally to this work.† Corresponding to chanx@microsoft.com1arXiv:2403.05751v2  [cs.LG]  16 Mar 2024Published as a conference paper at ICLR 2024diffusion models yield diverse samples from the conditional distributions, including possible low-fidelity samples from the low-density regions within the data manifold (Sehwag et al., 2022). Inthe context of time series forecasting, where fixed observations exclusively serve as objectives, suchvariability would result in forecasting instability and inferior prediction performance....ForwardprocessGuidanceSmoothingReverseprocessPredictionCoarser-grained DataCoarsest-grained DataCoarse-grained DataForward processGuidanceSmoothing outReverse process...ObservationReal DataDiffusion ProcessNoiseFigure 1: The process of smoothing datafrom finest-grained to coarsest-grained nat-urally aligns with the diffusion process.To stabilize the output of a diffusion model in timeseries prediction, one straightforward method is toconstrain the intermediate states during the sam-pling process. Prior research in the realm of dif-fusion models has introduced the idea of classifier-guidance (Nichol et al., 2021) and classifier-freeguidance (Ho & Salimans, 2022), where the pre-dicted posterior mean is shifted with the gradient ofeither explicit or implicit classifier. However, thesemethods require labels as the source of guidancewhile sampling, which are unavailable during out-of-sample inference. We observe that the forwardprocess of the diffusion model, which sequentiallycorrupts the data distribution to a standard normaldistribution, intuitively aligns with the process ofsmoothing fine-grained data into a coarser-grainedrepresentation, both of which result in a gradual lossof finer distribution features. This provides the in-sights that intrinsic features within data granularitiesmay also serve as a source of guidance.In this paper, we propose a novel Multi-Granularity Time Series Diffusion (MG-TSD) model thatleverages multiple granularity levels within data to guide the learning process of diffusion models.The coarse-grained data at different granularity levels are utilized as targets to guide the learning ofthe denoising process. These targets serve as constraints for the intermediate latent states, ensuring aregularized sampling path that preserves the trends and patterns within the coarse-grained data. Theyintroduce inductive bias which promotes the generation of coarser features during intermediate stepsand facilitates the recovery of finer features in subsequent diffusion steps. Consequently, this designreduces variability and results in high-quality predictions. Our key contributions can be summarizedas below:1. We introduce a novel MG-TSD model with an innovatively designed multi-granularityguidance loss function that efficiently guides the diffusion learning process, resulting inreliable sampling paths and more precise forecasting results.2. We provide a concise implementation that leverages coarse-grained data instances at var-ious granularity levels. Furthermore, we explore the optimal configuration for differentgranularity levels and propose a practical rule of thumb.3. Extensive experiments conducted on real-world datasets demonstrate the superiority of theproposed model, achieving the best performance compared to the state-of-the-art methods.2 B ACKGROUND2.1 D ENOISING DIFFUSION PROBABILISTIC MODELSSuppose x0 ∼ qX (x0) is a multivariate vector from spaceX = RD. Denoising diffusion probabilis-tic models aim to learn a model distribution pθ(x0) that approximates the data distribution q(x0).Briefly, they are latent variable models of the form pθ(x0) =Rpθ(x0:N )dx1:N , where xn for n =1, . . . , Nis a sequence of latent variables in the same sample space as x0. The denoising diffusionmodels are composed of two processes: the forward process and the reverse process. During the for-ward process, a small amount of Gaussian noise is added gradually inN steps to samples. It is char-models are composed of two processes: the forward process and the reverse process. During the for-ward process, a small amount of Gaussian noise is added gradually inN steps to samples. It is char-acterized by the following Markov chain:q(x1:N |x0) = QNn=1 q(xn|xn−1), where q(xn|xn−1) :=N(√1 − βnxn−1, βnI). The step sizes are controlled by a variance schedule {βn ∈ (0, 1)}Nn=1,where n represents a diffusion step. A nice property of the above process is that one can sample2Published as a conference paper at ICLR 2024at any arbitrary diffusion step in a closed form, let αn := 1 − βn and ¯αn = Qni=1 αi. It hasbeen shown that xn = √¯αnx0 + √1 − ¯αnϵ. The reverse diffusion process is to recreate the realsamples from a Gaussian noise input. It is defined as a Markov chain with learned Gaussian transi-tions starting with p(xN ) = N(xN ; 0, I). The reverse process is characterized as pθ(x0:N ) :=p(xN ) Q1n=N pθ(xn−1|xn), where pθ(xn−1|xn) := N(xn−1; µθ(xn, n), Σθ(xn, n)I); µθ :RD × N → RD and Σθ : RD × N → R+ take the variable xn ∈ RD and the diffusion stepn ∈ N as inputs, and share the parameters θ. The parameters in the model are optimized to mini-mize the negative log-likelihood minθ Ex0∼q(x0)[−log pθ(x0)] via a variational bound. Accordingto denoising diffusion probabilistic models (DDPM) in Ho et al. (2020), the parameterization ofpθ(xn−1|xn) is chosen as:µθ(xn, n) = 1√αnxn − 1 − αn√1 − ¯αnϵθ(√¯αnx0 +√1 − ¯αnϵ, n), (1)where ϵθ is a network which predicts ϵ ∼ N(0, I) from xn. We simplify the objective function intoLsimplen = En,ϵn,x0∥ϵn − ϵθ(√¯αnx0 +√1 − ¯αnϵn, n)∥2. (2)Once trained, we can iteratively sample from the reverse process pθ(xn−1|xn) to reconstruct x0.2.2 T IME GRAD MODELWe treat the time series forecasting task as a conditional generation task and utilize the diffusionmodels presented in Section 2.1 as the backbone generative model. TimeGrad model is a relatedwork by Rasul et al. (2020) which first explored the use of diffusion models for forecasting mul-tivariate time series. Consider a contiguous time series sampled from the complete history train-ing data, indexed from 1 to T. This time series is partitioned into a context window of interval[1, t0) and a prediction interval [t0, T]. TimeGrad utilizes diffusion models from Ho et al. (2020) tolearn the conditional distribution of the future timesteps of the multivariate time series given theirpast. An RNN is employed to capture the temporal dependencies, and the time series sequenceup to timestep t is encoded in the updated hidden state ht. Mathematically, TimeGrad modelsqX (xt0:T |x1:t0−1) = QTt=t0 qX (xt|x1:t−1) ≈ QTt=t0 qX (xt|ht−1), where xt ∈ RD denotes thetime series at timestep t and ht = RNNψ(xt, ht−1). Each factor is learned via a shared conditionaldenoising diffusion model. In contrast to Ho et al. (2020), the hidden states ht−1 are taken as anadditional input in the denoising network ϵθ(xnt , ht−1, n), and the loss function for timestep t anddiffusion step n is given by:Eϵ,x0,t,n[∥ϵ − ϵθ(√¯αnx0,t +√1 − ¯αnϵ, n,ht−1)∥2], (3)where the first subscript inx0,t represents the index of the diffusion step, whilet denotes the timestepwithin the time series.2.3 P ROBLEM FORMULATIONIn the time series prediction task, let X(1) represent the original observed data. The time seriesdata is denoted as X(1) = [ x11, . . . ,x1t , . . . ,x1T ], where t represents the timestep t ∈ [1, T] andxt ∈ RD. Specifically, our task is to model the conditional distribution of future timesteps of thetime series [x1t0 , . . . ,x1T ] given the fixed window of history context. Mathematically, the problemwe consider can be formulated as follows:qXx1t0:T |x11:t0−1	=TYt=t0qXx1t |x11:t−1	. (4)3 M ETHODIn this section, we provide an overview of theMG-TSD model architecture in Section 3.1, followedby a detailed discussion of the novel guided diffusion process module in Section 3.2, including thederivation of the heuristic loss function and its implementation across various granularity levels.3Published as a conference paper at ICLR 20243.1 MG-TSD MODEL ARCHITECTUREThe proposed methodology consists of three key modules, as depicted in Figure 2.Multi-granularity Data Generatoris responsible for generating multi-granularity data from ob-servations. In this module, various coarse-grained time series are obtained by smoothing out thefine-grained data using historical sliding windows with different sizes. Suppose f is a pre-definedsmoothing (for example, average) function, and sg is the pre-defined sliding window size for granu-larity levelg. Then X(g) = f(X(1), sg). The sliding windows are non-overlapping and the obtainedcoarse-grained data for granularity g are replicated sg times to align over the timeline [1, T].Temporal Process Moduleis designed to capture the temporal dynamics of the multi-granularitytime series data. We utilize RNN architecture on each granularity level g separately to encode thetime series sequence up to a specific timestep t and the encoded hidden states are denoted as hgt .The RNN cell type is implemented as GRU in Chung et al. (2014).Guided Diffusion Process Moduleis designed to generate stable time series predictions at eachtimestep t. We utilize multi-granularity data as given targets to guide the diffusion learning process.A detailed discussion of the module can be found in Section 3.2.Guided DiffusionProcess ModuleMulti-granularity Data GeneratorTPM...... ...... ...... ...... NoiseSampleTimestep Timestep(Temporal Process Module)0 DiffusionstepFigure 2: Overview of the Multi-Granularity Time Series Diffusion (MG-TSD) model, consisting ofthree key modules: Multi-granularity Data Generator, Temporal Process Module (TPM), andGuided Diffusion Process Modulefor time series forecasting at a specific granularity level.3.2 M ULTI -GRANULARITY GUIDED DIFFUSIONIn this section, we delve into the details of the Guided Diffusion Process Module, a key compo-nent in our model. Section 3.2.1 presents the derivation of a heuristic guidance loss for the two-granularity case. In Section 3.2.2, we generalize the loss to the multi-granularity case and providea concise implementation to effectively utilize coarse-grained data across various granularity levels.Briefly, the optimization of the heuristic loss function can be simply achieved by training denois-ing diffusion models on the multi-granularity data with shared denoising network parameters andpartially shared variance schedule.3.2.1 C OARSE -GRAINED GUIDANCEWithout loss of generality, consider two granularities: finest-grained data xg1t (g1 = 1) from X(g1)and coarse-grained data xgt from X(g) at a fixed timestep t, where 1 < t < T. We omit thesubscript t in the derivation for notation brevity. Suppose the denoising diffusion models presentedin Section 2.1 are employed to approximate the distribution q(xg1 ) and let the variance schedulebe {β1n = 1 − α1n ∈ (0, 1)}Nn=1. Suppose xg10 ∼ q(xg10 ), where the subscript 0 denotes the index4Published as a conference paper at ICLR 2024of diffusion step. The diffusion models in Section 2.2 define a forward trajectory q(xg10:N ) and aθ-parameterized reverse trajectory pθ(xg10:N ).While Section 2.2 focuses on predicting samples over a specific timestep, it does not account for theintrinsic structure of time series, such as trends, which are represented by coarse-grained time series.In this paper, we guide the generation of samples by ensuring that the intermediate latent spaceretains the underlying time series structure. This is achieved by introducing coarse-grained targetsxg at intermediate diffusion step Ng∗ ∈ [1, N− 1]. Specifically, we establish the objective functionas the log-likelihood of observed coarse-grained data xg evaluated at the marginal distributions atdiffusion step Ng∗ , which can be expressed as log pθ(xg). With an appropriate choice of diffusionstep Ng∗ , the coarser features recovered from the denoising process could gain information from therealistic coarse-grained sample. Recall that the marginal distribution of latent variable at denoisingstep Ng∗ determined by the θ-parameterized trajectory pθ(xNg∗ :N ) can be expressed as:pθ(xNg∗ ) =Zpθ(xNg∗ :N )dx(Ng∗ +1):N =Zp(xN )NYNg∗ +1pθ(xn−1|xn)dx(Ng∗ +1):N , (5)where xN ∼ N(0, I), pθ(xn−1|xn) = N(xn−1; µθ(xn, n), Σθ(xn, n)).To make the objective tractable, a common technique involves optimizing a variational lower boundon the likelihood in Equation 5. This can be achieved by specifying a latent variable sequenceof length N − Ng∗ , such that the joint distribution of xg and these latent variables is available.Conveniently, we employ a diffusion process onxg with a total of N −Ng∗ diffusion steps, defininga sequence of noisy samples xgNg∗ +1, . . ., xgN as realizations of the latent variable sequence. Then,the guidance objective can be expressed as:log pθ(xg) = logZpθ(xgNg∗, xgNg∗ +1, . . . ,xgN )dxg(Ng∗ +1):N . (6)Applying the same technique as in Ho et al. (2020), the guidance objective function in Equation 6simplifies the loss function of the diffusion models (see the Appendix A for proof details):Eϵ,xg,n[∥ϵ − ϵθ(xgn, n)∥2], (7)where xgn = (Qni=Ng∗α1i )xg +q1 − Qni=Ng∗α1i ϵ and ϵ ∼ N(0, I). When the variance scheduleis chosen as {α1n}Nn=Ng∗, the loss function of the diffusion model in Ho et al. (2020) is equivalent tothe guidance loss function presented in Equation 7.3.2.2 M ULTI -GRANULARITY GUIDANCEIn general, for G granularity levels, data of different granularities generated by Multi-granularityData Generatorcan be represented as X(1), X(2), . . . ,X(G). We expect these coarse-grained datacan guide the learning process of the diffusion model at different steps, serving as constraints alongthe sampling trajectory. For coarse-grained data at granularity level g, where g ∈ {2, . . . , G}, wedefine the share ratioas rg := 1 − (Ng∗ − 1)/N. It represents the shared percentage of varianceschedule between the gth granularity data and the finest-grained data. For the finest-grained data,N1∗ = 1 and r1 = 1. Formally, the variance schedule for granularity g is defined asαgn(Ng∗ ) =1 if n = 1, . . . , Ng∗α1n if n = Ng∗ + 1, . . . , N, (8)and {βgn}Nn=1 = {1 − αgn}Nn=1. Accordingly, define agn(Ng∗ ) = Qnk=1 αgk, and bgn(Ng∗ ) = 1 −agn(Ng∗ ). We suppose N1∗ < N2∗ . . . < Ng∗ < . . . < NG∗ , which represents the diffusion indexfor starting sharing the variance schedule across granularity level g ∈ {1, . . . , G}. The startingindex Ng∗ is larger for coarser granularity level, aligning with the intuition that the coarser-graineddata loses fine distribution features to a greater extent and is expected to resemble the samples fromearlier sampling steps.Furthermore, we use the temporal hidden states for granularity level g up to timestep t from theTemporal Process Moduleas conditional inputs for the model to generate time series at corre-Furthermore, we use the temporal hidden states for granularity level g up to timestep t from theTemporal Process Moduleas conditional inputs for the model to generate time series at corre-sponding granularity levels similar to Rasul et al. (2020). Then the guidance loss function L(g)(θ)5Published as a conference paper at ICLR 2024for gth-granularity xgn,t at timestep t and diffusion step n, can be expressed as:L(g)(θ) = Eϵ,xg0,t,n∥(ϵ − ϵθ(pagnxg0,t +pbgnϵ, n,hgt−1)∥22, (9)where hgt = RNNθ(xgt , hgt−1) is the updated hidden states from the last step.The guidance loss function with G − 1 granularity levels of data is Lguidance = PGg=2 ωgL(g)(θ),where ωg ∈ [0, 1] is a hyper-parameter controlling the scale of guidance from granularity g.Training. The training algorithm is in Algorithm 1. The final training objective is the weightedsummation of loss for all granularities, including the finest granularity:Lfinal = ω1L(1)(θ) + Lguidance(θ) =GXg=1ωgEϵ,xg0,t,n[∥ϵ − ϵθ(xgn,t, n,hgt−1)∥2], (10)where xgn,t =√agnxg0,t +√bgnϵ and PGg=1 ωg = 1. The denoising network parameters are sharedacross all granularities during training.Algorithm 1Training procedureInput: Context interval [1, t0); prediction interval [t0, T]; number of diffusion stepN; a set of shareratio for g granularity (or equivalently {Ng∗ , g∈ {1, . . . , G}}); generated multi-granularity data[xg1, . . . ,xgt0 , . . . ,xgT ], g∈ {1, . . . , G}; initial hidden states hg0, g∈ {1, . . . , G}]repeat1: Sample the multi-granularity time series [xg1, . . . ,xgT ], g∈ {1, . . . , G}.2: Obtain hgt = RNNg(xgt , hgt−1), g∈ {1, . . . , G}, t ∈ [1, . . . , T].3: for t = t0 to T do4: Initialize n ∼ Uniform(1, . . . , N) and ϵ ∼ N(0, I)5: Reset the variance schedule {βgn = 1 − αgn(Ng∗ )}Nn=1, g ∈ {1, . . . , G}.6: Compute loss Lfinal according to Equation 107: Take the gradient ∇θLfinal8: end foruntil convergedInference. Once the model is trained, our goal is to make predictions on the finest-grained data, upto a certain number of future prediction steps. Suppose that the last context window ends at timestept0 − 1, we use Algorithm 2 to perform the sampling procedure and generate a sample x1t0 for thenext timestep. This process is repeated until reaching the desired forecast horizon. With differenthidden states as conditional inputs, the model can sample time series at respective granularity levels.Algorithm 2Inference procedure for each timestep t ∈ [t0, T]Input: Noise xNt ∼ N(0, I) and hidden states hgt−1, g ∈ {1, . . . , G}1: for n = N to 1 do2: if n >1 then3: Sample z ∼ N(0, I)4: else5: z = 06: end if7: for g = 1 to G do8: xgn−1,t = 1√αgn(xgn,t − βgn√1−agnϵθ(xgn,t, n,hgt−1)) +√σgnz, where σgn =1−agn−11−agnβgn.9: end for10: end forReturn: xg0,t, g= 1(finest-grained data); (Optional: xg0,t, g ∈ {2, . . . , G})Selection of share ratio.We propose a heuristic approach to help select the appropriate share ratiorg, which is derived from Ng∗ . We determine the choice of Ng∗ as the diffusion step at which thedistance between two distributions q(xg) and pθ(xg1n ) is minimum, as shown below:Ng∗ := arg minnD(q(xg), pθ(xg1n )), (11)6Published as a conference paper at ICLR 2024where D is a measure for accessing discrepancy between two distributions, such as KL divergence.In practice, we first pre-train a TimeGrad model and then compute the CRPSsum between the coarse-grained targets and the samples along the sampling path of finest-grained data during inference. Therange of steps where the CRPSsum values can consistently maintain relatively small values suggestsa proper range of share ratios.4 E XPERIMENTSIn this section, we conduct extensive experiments on six real-world datasets to evaluate the perfor-mance of the proposed MG-TSD model and compare it with previous state-of-the-art baselines.4.1 S ETTINGSDatasets. We consider six real-word datasets characterized by a range of temporal dynamics,namely Solar, Electricity, Traffic, Taxi, KDD-cup and Wikipedia. The data isrecorded at intervals of 30 minutes, 1 hour, or 1 day frequencies. Refer to Appendix C.1 for details.Evaluation Metrics. We assess our model and all baselines using CRPS sum (Continuous RankedProbability Score), a widely used metric for probabilistic time series forecasting, as well asNMAEsum (Normalized Mean Absolute Error) and NRMSE sum (Normalized Root Mean SquaredError). For detailed descriptions, refer to Appendix D.Baselines. We assess the predictive performance of the proposed MG-TSD model in comparisonwith multivariate time series forecasting models, including Vec-LSTM-ind-scaling (Salinas et al.,2019), GP-scaling (Salinas et al., 2019), GP-Copula (Salinas et al., 2019), Transformer-MAF (Rasulet al., 2020), LSTM-MAF (Rasul et al., 2020), TimeGrad (Rasul et al., 2021), and TACTiS (Drouinet al., 2022). The MG-Input ensemble model serves as the baseline with multi-granularity inputs. Itcombines two TimeGrad models trained on one coarse-grained and finest-grained data respectively,and generates the final predictions by a weighted average of their outputs.Implementation details.We train our model for 30 epochs using the Adam optimizer with a fixedlearning rate of 10−5. We set the mini-batch size to 128 for solar and 32 for other datasets. Thediffusion step is configured as 100. Additional hyper-parameters, such as share ratios, granularitylevels, and loss weights, are detailed in Appendix C.3. All models are trained and tested on a singleNVIDIA A100 80GB GPU.4.2 R ESULTSThe CRPSsum values averaged over 10 independent runs are reported in Table 1. The results showour model achieves the lowest CRPSsum and outperforms the baseline models across all six datasets.The MG-Input model exhibits marginal improvement on certain datasets when compared to theTimeGrad. This implies that while integrating multi-granularity information may result in someinformation gain, direct ensembling of coarse-grained outputs is inefficient in boosting performance.4.3 A BLATION STUDYShare ratio of variance schedule.To investigate the effect of share ratio, we evaluate the perfor-mance of MG-TSD using various share ratios across different coarse granularities. The experimentis conducted in a two-granularity setting, where one coarse granularity is utilized to guide the learn-ing process for the finest-grained data. Table 2 shows that for each coarse granularity level, theCRPSsum values initially decrease to their lowest values and then ascend again as the share ratiogets larger. Furthermore, we observe for coarser granularities, the model performs better with asmaller share ratio. This suggests that the model achieves optimal performance when the share ratiois chosen at the step where the coarse-grained samples most closely resemble intermediate states.Utilizing 4-hour or 6-hour granularity as guidance greatly enhances the model performance. How-ever, the improvement in performance diminishes as the granularity becomes coarser, such as 12hours or 24 hours, possibly due to the greater loss of information on local fluctuations.ever, the improvement in performance diminishes as the granularity becomes coarser, such as 12hours or 24 hours, possibly due to the greater loss of information on local fluctuations.In practice, the selection of share ratio can follow the heuristic rule outlined in Section 3.2.2. Fig-ure 3 provides illustrative plots for the share ratio selection curve of different granularities. The blue7Published as a conference paper at ICLR 2024Table 1: Comparison of CRPS sum (smaller is better) of models on six real-world datasets. Thereported mean and standard error are obtained from 10 re-training and evaluation independent runs.Method Solar Electricity Traffic KDD-cup Taxi WikipediaVec-LSTM ind-scaling0.4825±0.0027 0.0949±0.0175 0.0915±0.0197 0.3560±0.1667 0.4794±0.0343 0.1254±0.0174GP-Scaling 0.3802±0.0052 0.0499±0.0031 0.0753±0.0152 0.2983±0.0448 0.2265±0.0210 0.1351±0.0612GP-Copula 0.3612±0.0035 0.0287±0.0005 0.0618±0.0018 0.3157±0.0462 0.1894±0.0087 0.0669±0.0009LSTM-MAF 0.3427±0.0082 0.0312±0.0046 0.0526±0.0021 0.2919±0.1486 0.2295±0.0082 0.0763±0.0051Transformer-MAF0.3532±0.0053 0.0272±0.0017 0.0499±0.0011 0.2951±0.0504 0.1531±0.0038 0.0644±0.0037TimeGrad 0.3335±0.0653 0.0232±0.0035 0.0414±0.0112 0.2902±0.2178 0.1255±0.0207 0.0555±0.0088TACTiS 0.4209±0.0330 0.0259±0.0019 0.1093±0.0076 0.5406±0.1584 0.2070±0.0159 −MG-Input 0.3239±0.0427 0.0238±0.0035 0.0658±0.0065 0.2977±0.1163 0.1592±0.0087 0.0567±0.0091MG-TSD 0.3081±0.0099 0.0149±0.0017 0.0323±0.0125 0.1837±0.0865 0.1159±0.0132 0.0529±0.0054Table 2: Influence of share ratios for different granularities on Solar dataset. The reported meanand standard error are obtained from 10 re-training and evaluation independent runs.Ratio 4 hour 6 hourCRPSsum NMAEsum NRMSEsum CRPSsum NMAEsum NRMSEsum20% 0.3489±0.0190 0.3826±0.0200 0.7177±0.0445 0.3378±0.0305 0.3703±0.0368 0.6916±0.053640% 0.3405±0.0415 0.3792±0.0386 0.6870±0.0870 0.3275±0.0250 0.3608±0.0267 0.6650±0.037460% 0.3268±0.0475 0.3604±0.0463 0.6579±0.0919 0.3166±0.0376 0.3491±0.0368 0.6478±0.069680% 0.3172±0.0249 0.3510±0.0240 0.6515±0.051 0.3221±0.0425 0.3555±0.0443 0.6542±0.0747100% 0.3178±0.0342 0.3480±0.0356 0.6591±0.0503 0.3232±0.0396 0.3548±0.0417 0.6550±0.0660Ratio 12 hour 24 hourCRPSsum NMAEsum NRMSEsum CRPSsum NMAEsum NRMSEsum20% 0.3440±0.0391 0.3767±0.0450 0.6999±0.0772 0.3315±0.0266 0.3693±0.0298 0.6801±0.055440% 0.3374±0.0370 0.3713±0.0346 0.6837±0.0641 0.3276±0.0358 0.3612±0.0361 0.6722±0.055260% 0.3240±0.0382 0.3597±0.0388 0.6694±0.0746 0.3382±0.0343 0.3737±0.0365 0.6878±0.065580% 0.3391±0.0390 0.3719±0.0403 0.6953±0.0691 0.3288±0.0460 0.3639±0.0476 0.6741±0.0929100% 0.3284±0.0323 0.3538±0.0450 0.6609±0.0917 0.3407±0.0248 0.3692±0.0244 0.6933±0.0528curve in each plot represents CRPS sum values between coarse-grained targets and 1-hour samplescome from 1-gran(finest-gran) model at each intermediate denoising step; each point on the orangepolylines represents the CRPSsum value of 1-hour predictions by 2-gran MG-TSD models with dif-ferent share ratios ranging from [0.2, 0.4, 0.6, 0.8, 1.0], and the lowest point of the line segment canbe used to characterize the most suitable share ratio for the corresponding granularity.The diffusion steps that can achieve relatively small CRPS sum values are colored in grey, suggest-ing a proper range for the share ratio at which the model can achieve satisfactory performance.From the plots, a strong correlation exists between the polyline of CRPS sum calculated during testtime and the share ratio selection curve, which validates the effectiveness of the selection rule. Inaddition, as granularity transitions from fine to coarse (4h →6h→12h→24h), the diffusion steps atwhich the distribution most resembles the coarse-grained targets increase (approximately at steps20→40→60→60). This comparison shows the similarity between the diffusion process and thesmoothing process from the finest-grained to coarse-grained data, both of which involve a gradualloss of finer characteristics from the finest-grained data through a smooth and convex transformation.(a) CRPSsum values between 4-hour targets and samples(b) CRPSsum values between 6-hour targets and samples(c) CRPSsum values between 12-hour targets and samples(d) CRPSsum values between 24h-granularity targets and samplesFigure 3: Selection of share ratio for MG-TSD modelshour targets and samples(c) CRPSsum values between 12-hour targets and samples(d) CRPSsum values between 24h-granularity targets and samplesFigure 3: Selection of share ratio for MG-TSD modelsThe number of granularities.We further explore the impact of the number of granularities onthe MG-TSD model. As presented in Table 3, increasing the number of granularity levels gener-ally boosts the performance of the MG-TSD model, which demonstrates that the introduction of8Published as a conference paper at ICLR 2024multi-granularity information effectively guides the learning process of diffusion models. However,the marginal benefit diminishes with the increase in granularity amounts. The results suggest thatutilizing four to five granularity levels generally suffices for achieving favorable performance.Table 3: Influence of the number of granularities on MG-TSD performance for Solar andElectricity Dataset.Num of gran Solar ElectricityCRPSsum NMAEsum NRMSEsum CRPSsum NMAEsum NRMSEsum2 0.3172±0.0249 0.3510±0.0240 0.6515±0.0571 0.0174±0.0042 0.0226±0.0071 0.0296±0.00863 0.3110±0.0329 0.3494±0.0378 0.6452±0.0632 0.0160±0.0020 0.0198±0.0029 0.0262±0.00394 0.3081±0.0099 0.3445±0.0102 0.6245±0.0268 0.0149±0.0017 0.0178±0.0018 0.0241±0.00305 0.3093±0.0411 0.3430±0.0451 0.6117±0.0746 0.0153±0.0027 0.0181±0.0043 0.0254±0.0058Num of gran Traffic KDD-cupCRPSsum NMAEsum NRMSEsum CRPSsum NMAEsum NRMSEsum2 0.0347±0.0020 0.0396±0.0022 0.0593±0.0043 0.2427±0.1167 0.3171±0.1557 0.3745±0.16523 0.0334±0.0034 0.0382±0.0035 0.0574±0.0066 0.2414±0.1619 0.3030±0.1789 0.3808±0.21684 0.0326±0.0041 0.0374±0.0048 0.0573±0.0050 0.2198±0.1162 0.2893±0.1554 0.3315±0.18825 0.0323±0.0125 0.0370±0.0140 0.0563±0.0230 0.1837±0.0636 0.2463±0.0865 0.3001±0.09974.4 C ASE STUDYTo illustrate the guidance effect of coarse-grained data, we visualize the ground truth and the pre-dicted mean for both 1-hour and 4-hour granularity time series across four dimensions in theSolardataset in Figure 4. For comparison, the prediction results for the 1-hour data from TimeGrad arealso included. The results indicate that the TimeGrad model struggles to accurately capture thepeaks in the series and tends to underestimate the peaks in solar energy. In theMG-TSD model, thecoarse-grained samples display a more robust capacity to capture the trends, subsequently guidingthe generation of more precise fine-grained data.00:00 00:00 00:0006:0012:0018:00 06:0012:0018:00050100150200250Dim A00:00 00:00 00:0006:0012:0018:00 06:0012:0018:00050100150200Dim B00:00 00:00 00:0006:0012:0018:00 06:0012:0018:00020406080100120Dim C00:00 00:00 00:0006:0012:0018:00 06:0012:0018:000204060Dim DObservationsTimeGrad: median predictionMG-TSD: median prediction of 1hMG-TSD: median prediction of 4hTimeGrad: 50.0% prediction intervalMG-TSD: 50.0% prediction intervalFigure 4: Visualization of the ground-truth ( Solar dataset), MG-TSD predicted mean for 4-hourand 1-hour time series, and TimeGrad predicted mean for the 1-hour time series. Additionally, the50% prediction intervals for the 1-hour data are also included. These plots represent some illustrativedimensions out of 370 dimensions from the first 24-hour rolling-window.5 C ONCLUSIONIn this paper, we introduce a novel Multi-Granularity Time Series Diffusion (MG-TSD) model,which leverages the inherent granularity levels within the data as given targets at intermediate dif-fusion steps to guide the learning process of diffusion models. We derive a novel multi-granularityguidance diffusion loss function and propose a concise implementation method to effectively utilizecoarse-grained data across various granularity levels. Extensive experiments conducted on real-world datasets demonstrate that MG-TSD outperforms existing time series prediction methods.9Published as a conference paper at ICLR 2024REFERENCESAlexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, JanGasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, JasperSchulz, Lorenzo Stella, Ali Caner T ¨urkmen, and Yuyang Wang. GluonTS: Probabilistic andNeural Time Series Modeling in Python. Journal of Machine Learning Research , 21(116):1–6,2020a. URL http://jmlr.org/papers/v21/19-820.html.Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, JanGasthaus, Tim Januschowski, Danielle C Maddix, Syama Rangapuram, David Salinas, JasperSchulz, et al. Gluonts: Probabilistic and neural time series modeling in python. The Journal ofMachine Learning Research, 21(1):4629–4634, 2020b.Amulya Arun Ballakur and Arti Arya. Empirical evaluation of gated recurrent neural network ar-chitectures in aviation delay prediction. In 2020 5th International Conference on Computing,Communication and Security (ICCCS), pp. 1–7, 2020. doi: 10.1109/ICCCS49678.2020.9276855.Yingmei Chen, Zhongyu Wei, and Xuanjing Huang. Incorporating corporation relationship viagraph convolutional neural networks for stock price prediction. In Proceedings of the 27th ACMInternational Conference on Information and Knowledge Management, pp. 1655–1658, 2018.Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation ofgated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.Emmanuel de B ´ezenac, Syama Sundar Rangapuram, Konstantinos Benidis, Michael Bohlke-Schneider, Richard Kurle, Lorenzo Stella, Hilaf Hasson, Patrick Gallinari, and Tim Januschowski.Normalizing kalman filters for multivariate time series analysis. Advances in Neural InformationProcessing Systems, 33:2995–3007, 2020.Alexandre Drouin, ´Etienne Marcotte, and Nicolas Chapados. Tactis: Transformer-attentional cop-ulas for time series. In International Conference on Machine Learning, pp. 5447–5493. PMLR,2022.Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprintarXiv:2207.12598, 2022.Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances inNeural Information Processing Systems, 33:6840–6851, 2020.Sepp Hochreiter and J ¨urgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735–1780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.Min Hou, Chang Xu, Yang Liu, Weiqing Liu, Jiang Bian, Le Wu, Zhi Li, Enhong Chen, and Tie-Yan Liu. Stock trend prediction with multi-granularity data: A contrastive learning approach withadaptive fusion. In Proceedings of the 30th ACM International Conference on Information &Knowledge Management, pp. 700–709, 2021.Irena Koprinska, Dengsong Wu, and Zheng Wang. Convolutional neural networks for energy timeseries forecasting. In 2018 international joint conference on neural networks (IJCNN) , pp. 1–8.IEEE, 2018.Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling Long- and Short-TermTemporal Patterns with Deep Neural Networks. InThe 41st International ACM SIGIR Conferenceon Research & Development in Information Retrieval , SIGIR ’18, pp. 95–104, New York, NY ,USA, June 2018. Association for Computing Machinery. ISBN 978-1-4503-5657-2. doi: 10.1145/3209978.3210006.Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. Generative time series forecasting with dif-fusion, denoise, and disentanglement. Advances in Neural Information Processing Systems , 35:23009–23022, 2022.10Published as a conference paper at ICLR 2024Junyu Luo, Muchao Ye, Cao Xiao, and Fenglong Ma. Hitanet: Hierarchical time-aware atten-tion networks for risk prediction on electronic health records. In Proceedings of the 26th ACMSIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 647–656, 2020.Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing withtext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64words: Long-term forecasting with transformers. In The Eleventh International Conference onLearning Representations, 2022.George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for densityestimation. Advances in neural information processing systems, 30, 2017.Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.Pranav Rajpurkar, Emma Chen, Oishi Banerjee, and Eric J Topol. AI in health and medicine.Naturemedicine, 28(1):31–38, 2022.Kashif Rasul. PytorchTS, 2021. URL https://github.com/zalandoresearch/pytorch-ts.Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs Bergmann, and Roland V ollgraf. Mul-tivariate probabilistic time series forecasting via conditioned normalizing flows. arXiv preprintarXiv:2002.06103, 2020.Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland V ollgraf. Autoregressive denoising dif-fusion models for multivariate probabilistic time series forecasting. In International Conferenceon Machine Learning, pp. 8857–8868. PMLR, 2021.David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus.High-dimensional multivariate forecasting with low-rank gaussian copula processes. Advancesin neural information processing systems, 32, 2019.Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Ozgenel, and Cristian Canton. Generating highfidelity data from low-density regions using diffusion models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 11492–11501, 2022.Lifeng Shen and James Kwok. Non-autoregressive conditional diffusion models for time seriesprediction. In International Conference on Machine Learning, pp. 31016–31029. PMLR, 2023.Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-baseddiffusion models for probabilistic time series imputation. Advances in Neural Information Pro-cessing Systems, 34:24804–24816, 2021.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V onLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-vances in Neural Information Processing Systems , volume 30. Curran Associates, Inc.,2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition trans-formers with auto-correlation for long-term series forecasting. Advances in neural informationprocessing systems, 34:22419–22430, 2021.Haixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. Interpretable weather forecasting forworldwide stations with a unified deep model. Nature Machine Intelligence, pp. 1–10, 2023.11Published as a conference paper at ICLR 2024Peter T Yamak, Li Yujian, and Pius K Gadosey. A comparison between arima, lstm, and gru fortime series forecasting. In Proceedings of the 2019 2nd international conference on algorithms,computing and artificial intelligence, pp. 49–55, 2019.Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. Proceed-ings of the AAAI Conference on Artificial Intelligence , 35(12):11106–11115, May 2021. ISSN2374-3468. doi: 10.1609/aaai.v35i12.17325.Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequencyenhanced decomposed transformer for long-term series forecasting. In International Conferenceon Machine Learning, pp. 27268–27286. PMLR, 2022.12Published as a conference paper at ICLR 2024A A PPENDIX : D ERIVATION OF LOSS FUNCTIONRecall that we specify a sequence of noisy samplesxgNg∗ +1, . . . ,xgN by applying the forward processon xg. The superscript in Ng∗ is suppressed for notation brevity. Suppose the coarse-grained dataxgN∗ ∼ q(xg), where the subscript notation N∗ indicates that the observed xg is treated as a samplefrom the distribution. (In the diffusion model, the subscript is typically denoted as 0, but we startwith N∗ to simplify the derivation).log pθ(xgN∗) ≤ −log pθ(xgN∗) + DKL(q(xg(N∗+1):N |xgN∗)∥pθ(xg(N∗+1):N |xgN∗))= −log pθ(xgN∗) + Exg(N∗+1):N∼q(xg(N∗+1):N|xgN∗)hlogq(xg(N∗+1):N |xgN∗)pθ(xgN∗:N )/pθ(xgN∗)i= −log pθ(xgN∗) + Eqhlogq(xg(N∗+1):N |xgN∗)pθ(xgN∗:N ) + logpθ(xgN∗)i= Eqhlogq(xg(N∗+1):N |xgN∗)pθ(xgN∗:N )i(12)Then, the training objective can be performed by optimizing the usual variational lower bound shownbelow:LVLB = Eq(xgN∗:N)hlogq(xg(N∗+1):N |xgN∗)pθ(xgN∗:N )i≥ −Eq(xgN∗) log pθ(xgN∗) (13)It is obvious that the objective LVLB is equivalent to the that of diffusion model in Ho et al. (2020)when employing diffusion models on xg with N − N∗ steps. The forward process is defined asq(xg(N∗+1):N |xgN∗) = QNn=N∗ q(xgn|xgn−1), where q(xgn|xgn−1) := N(p1 − βgnxgn−1, βgnI). The{βgn}Nn=N∗ share values with the variance schedule {β1n}Nn=1 of the finest-grained data from indexN∗. And, the reverse process is defined by the θ-parameterized trajectory. Then following the sametechnique in Ho et al. (2020), the LVLB can reduce to the usual loss of diffusion models.B A PPENDIX : E XPERIMENTSB.1 B ENCHMARK EXPERIMENTSThe results of the benchmark experiments, evaluated based on the metrics NRMSE sum andNMAEsum, are presented in Table 4 and Table 5 respectively. In the experiments, we include fourextra baseline models for a more comprehensive comparison: TimeDiff (Shen & Kwok, 2023),D3V AE (Li et al., 2022), PatchTST (Nie et al., 2022), and AutoFormer (Wu et al., 2021).Table 4: Comparison of NRMSE sum (smaller is better) of models on six real-world datasets. Thereported mean and standard error are obtained from 10 re-training and evaluation independent runs.Method Solar Electricity Traffic KDD-cup Taxi WikipediaVec-LSTM ind-scaling0.9952±0.0077 0.1439±0.0228 0.1451±0.0248 0.4461±0.1833 0.6398±0.0390 0.1618±0.0162GP-Scaling 0.9004±0.0095 0.0811±0.0062 0.1469±0.0181 0.3445±0.0621 0.3598±0.0285 0.1710±0.1006GP-Copula 0.8279±0.0053 0.0512±0.0009 0.1282±0.0033 0.2605±0.0227 0.3125±0.0113 0.0930±0.0076Autoformer 0.7046±0.0000 0.0475±0.0000 0.0951±0.0000 0.8984±0.0000 0.3498±0.0000 0.1052±0.0000PatchTST 0.7270±0.0000 0.0474±0.0000 0.1897±0.0000 0.5137±0.0000 0.3690±0.0000 0.0915±0.0000D3V AE 0.7472±0.0508 0.1640±0.0928 0.4722±0.1197 0.5628±0.0419 0.7624±0.5598 2.2094±2.1646TimeDiff 1.5985±0.0359 0.3714±0.0073 0.5520±0.0087 0.4955±0.0147 0.5479±0.0084 0.1412±0.0099TimeGrad 0.6953±0.0845 0.0348±0.0057 0.0653±0.0244 0.4092±0.1332 0.2365±0.0386 0.0870±0.0106TACTiS 0.8532±0.0851 0.0427±0.0023 0.2270±0.0159 0.6513±0.1767 0.3387±0.0097 -MG-TSD 0.6178±0.0418 0.0241±0.0030 0.0563±0.0230 0.3001±0.0997 0.2334±0.0313 0.0810±0.005713Published as a conference paper at ICLR 2024Table 5: Comparison of NMAE sum (smaller is better) of models on six real-world datasets. Thereported mean and standard error are obtained from 10 re-training and evaluation independent runs.Method Solar Electricity Traffic KDD-cup Taxi WikipediaVec-LSTM ind-scaling0.5091±0.0027 0.1261±0.0211 0.1042±0.0228 0.4193±0.1902 0.4974±0.0351 0.1416±0.0180GP-Scaling 0.4945±0.0065 0.0648±0.0046 0.0975±0.0163 0.2892±0.0550 0.2867±0.0264 0.1452±0.1029GP-Copula 0.4302±0.0046 0.0312±0.0007 0.0769±0.0022 0.2140±0.0124 0.2390±0.0098 0.0659±0.0061Autoformer 0.6368±0.0000 0.0388±0.0000 0.0684±0.0000 0.7658±0.0000 0.2652±0.0000 0.1239±0.0000PatchTST 0.4351±0.0000 0.0350±0.0000 0.1219±0.0000 0.4497±0.0000 0.2887±0.0000 0.0625±0.0000D3V AE 0.4457±0.0377 0.1434±0.0892 0.3992±0.1177 0.4874±0.0520 0.6080±0.5061 2.0151±2.0005TimeDiff 1.3343±0.0305 0.3519±0.0075 0.4782±0.0058 0.3630±0.0127 0.4521±0.0102 0.1146±0.0106TimeGrad 0.3694±0.0400 0.0266±0.0049 0.0410±0.0089 0.3614±0.1334 0.1365±0.0193 0.0631±0.008TACTiS 0.4448±0.0313 0.0310±0.0015 0.1352±0.0159 0.6078±0.1718 0.2244±0.0036 -MG-TSD 0.3347±0.0220 0.0178±0.0018 0.0370±0.0140 0.2463±0.0865 0.1300±0.0150 0.0601±0.0057B.2 M ORE EXPERIMENT SETTINGSB.2.1 P ERFORMANCE FOR LONG -TERM FORECASTINGTo evaluate the performance of MG-TSD for long-term forecasting, we maintain a fixed contextlength of 24 and extend the prediction length to 24, 48, 96, and 144. The results of the datasetsSolar and Eelectrity are displayed in Figure 5.The results in Figure 5 indicate that MG-TSD performs well for long-time forecasting. The resultsindicate that as the prediction length increases, the performance of our proposed method stays robust,exhibiting no sudden decline. Furthermore, our method consistently outperforms the competitivebaseline. This performance advantage is anticipated to persist in future trends, with no indication ofconvergence between the approaches.(a)Solar: Performance Evaluation Across Different Prediction Lengths(b)Electricity: Performance Evaluation Across Different Prediction LengthsFigure 5: Performance evaluation across different prediction horizons for MG-TSD with TimeGradas the baseline Model. The context length is fixed at 24h and the prediction length is tested at24h, 48h, 96h, and 144h. The average CRPS, NRMSE, and NMAE metrics are computed for bothMG-TSD and the baseline over 10 independent runs, with error bars indicating the correspondingstandard deviations.B.2.2 T IME AND MEMORY USAGE OF THE MG-TSD MODEL DURING TRAININGExperiments have been conducted to evaluate the time and memory usage of the MG-TSD modelduring training across various granularities. These experiments were executed using a single A6000card with 48G memory capacity. The Solar dataset was utilized in this context, with a batch size of128, an input size of 552, 100 diffusion steps, and 30 epochs.As illustrated in Figure 6, there is a linear increase in memory consumption with an increase ingranularity. A slight surge in training time is also observed. These findings are coherent with thearchitecture of our model. In particular, each additional granularity results in the introduction ofan extra RNN in the Temporal Process Module and an increase in computation within the GuidedDiffusion Process Module. As per theoretical expectations, these resource consumptions shouldexhibit linear growth. The slight increase in training time can be ascribed to the design of theMulti-granularity Data Generator Module which enables parallel forward processes across differentgranularities, thus promoting acceleration. Moreover, it is pertinent to mention that an excessiveincrease in granularity may not notably boost the final prediction results, hence the granularity willbe kept within a certain range. Therefore, the consumption of memory will not rise indefinitely.14Published as a conference paper at ICLR 20242 3 4 5Granularity51015202530Memory(GB)/ Run Time(min) MemoryRun TimeFigure 6: Comparison of Time and Memory Consumption at Different Granularity Levels in MG-TSD Model TrainingB.2.3 V ARIATIONS IN THE FREQUENCY DOMAIN OF TIME SERIES DATA: T HE IMPACT OFGRANULARITY AND DENOISING STEPSWe sampled series from Solar dateset and we conducted a Fast Fourier Transform to extract the sea-sonality components of the series, as well as the samples of different granularities and correspondingnoisy samples along the forward diffusion process.0 25 50 75 100 125 150Frequency0500100015002000250030003500Amplitudegran: 1hgran: 4hgran: 6hgran: 12hgran: 24h(a) Different granularities0 25 50 75 100 125 150Frequency0500100015002000250030003500Amplitudediffusion step: 0diffusion step: 20diffusion step: 40diffusion step: 60diffusion step: 100 (b) Different diffusion stepsFigure 7: Variations in the frequency domain of time series data: the impact of granularity anddenoising steps.As depicted in Figure 7(a), as granularity becomes coarser, the components of all outstanding fre-quencies get lower, while the high-frequency peak (around 125 and 80) diminishes quicker thanlower-frequency peak (around 45). Figure 7(b) demonstrates the distribution of frequency compo-nents of the same noisy series with gradually ascending forward diffusion steps and the same patternis observable. This empirical study indicates the connection between the forward diffusion processand the smoothing process from fine-grained data to coarse-grained data, both of which result inlosing finer informative features.C A PPENDIX : I MPLEMENTATION DETAILSC.1 B ENCHMARK DATASETSFor our experiments, we use Solar, Electricity, Traffic, Taxi, KDD-cup andWikipedia open-source datasets, with their properties listed in Table 6.The dataset can be obtained through the links below.(i) Solar: https://www.nrel.gov/grid/solar-power-data.html15Published as a conference paper at ICLR 2024(ii) Electricity: https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014(iii) Traffic: https://archive.ics.uci.edu/dataset/204/pems+sf(iv) Taxi: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page(v) KDD-cup: https://www.kdd.org/kdd2018/kdd-cup(vi) Wikipedia: https://github.com/mbohlkeschneider/gluon-ts/tree/mv_release/datasetsName Frequency Number of series Context length Prediction length Multi-granularity dictionarySolar 1 hour 137 24 24 [1 hour, 4 hour, 12 hour, 24hour, 48 hour]Electricity1 hour 370 24 24 [1 hour, 4 hour, 12 hour, 24 hour, 48 hour]Traffic 1 hour 963 24 24 [1 hour, 4 hour, 12 hour, 24 hour, 48 hour]Taxi 30 min 1214 24 24 [30 min , 2 hour, 6 hour, 12 hour, 24 hour]KDD-cup 1 hour 270 48 48 [1 hour, 4 hour, 12 hour, 24hour, 48 hour]Wikipedia 1 day 2000 30 30 [1 day, 4 day, 7 day, 14 day]Table 6: Detailed information of the datasets used in our benchmark including data frequency andnumber of times series (dimension), including the information about context length and predictionlength and the multi-granularity dictionary utilized in the multivariate time series forecasting task.C.2 L IBRARIES USEDThe MG-TSD code in this study is implemented using PyTorch (Paszke et al., 2019). It utilizes thePytorchTS library (Rasul, 2021), which enables convenient integration of PyTorch models with theGluonTS library (Alexandrov et al., 2020b) on which we heavily rely for data preprocessing, modeltraining, and evaluation in our experiments.The code for the baseline methods is obtained from the following sources.(i) Vec-LSTM-ind-scaling: models the dynamics via an RNN and outputs the parameters of anindependent Gaussian distribution with mean-scaling.Code: https://github.com/mbohlkeschneider/gluon-ts/tree/mv_release;(ii) GP-scaling: a model that unrolls an LSTM with scaling on each individual time series beforereconstructing the joint distribution via a low-rank Gaussian.Code: https://github.com/mbohlkeschneider/gluon-ts/tree/mv_release(iii) GP-Copula: a model that unrolls an LSTM on each individual time series. The joint emissiondistribution is then represented by a low-rank plus diagonal covariance Gaussian copula.Code: https://github.com/mbohlkeschneider/gluon-ts/tree/mv_release;(iv) LSTM-MAF: a model which utilizes LSTM for modeling the temporal conditioning and em-ploys Masked Autoregressive Flow (Papamakarios et al., 2017) for the distribution emission.Code: https://github.com/zalandoresearch/pytorch-ts/tree/master/pts/model/tempflow(v) Transformer-MAF: a model which utilizes Transformer (Vaswani et al., 2017) for modeling thetemporal conditioning and employs Masked Autoregressive Flow (Papamakarios et al., 2017) forthe distribution emission model.Code: https://github.com/zalandoresearch/pytorch-ts/tree/master/pts/model/transformer_tempflow(vi) TimeGrad: an auto-regressive model designed for multivariate probabilistic time series fore-casting, assisted by an energy-based model.Code: https://github.com/zalandoresearch/pytorch-ts(vii) TACTiS: a non-parametric copula model based on transformer architecture.Code: https://github.com/servicenow/tactis16Published as a conference paper at ICLR 2024(viii) D3V AE: a bidirectional variational auto-encoder(BV AE) equipped with diffusion, denoise, anddisentanglement.Code: https://github.com/ramber1836/d3vae.(ix) TimeDiff: a predictive framework trained by blending hidden contextual elements with futureactual outcomes for sample conditioning.Code: There is no publicly available code; we obtained the code by emailing the author.(x) Autoformer: redefines the Transformer with a deep decomposition architecture, including se-quence decomposition units, self-correlation mechanisms, and encoder-decoders.Code: https://github.com/thuml/Autoformer(xi) PatchTST: an efficient design of Transformer-based models for multivariate time series fore-casting and self-supervised representation learning.Code: https://github.com/yuqinie98/PatchTSTC.3 H YPER -PARAMETER SETTING FOR EACH MODELDataset Num gran Gran dict Share ratio Loss weightSolarElectricityTrafficKDD-cup2 [1h,4h][1h,12h][1,0.9][0.9,0.1][1,0.8][1,0.6]3 [1h,4h,12h][1h,4h,24h][1,0.9,0.8] [0.8, 0.1, 0.1][1,0.8,0.8] [0.9, 0.05, 0.05][1,0.8,0.6] [0.85, 0.10, 0.05]4 [1h,4h,12h,24h][1h,4h,12h,48h][1,0.9,0.8,0.8][0.8, 0.1, 0.05, 0.05][0.7,0.1,0.1,0.1][1,0.9,0.8,0.6][1,0.8,0.6,0.6][1,0.8,0.6,0.4]5 [1h,4h,8h,12h,24h][1h,4h,12h,24h,48h][1,0.9,0.8,0.6,0.6][0.8,0.1,0.05,0.04,0.01][0.8,0.05,0.05,0.05,0.05][0.6,0.1,0.1,0.1,0.1][1,0.9,0.8,0.6,0.4][1,0.8,0.6,0.6,0.6][1,0.8,0.6,0.6,0.4][1,0.8,0.6,0.4,0.4]Taxi 2[30m,2h][1,0.8][1,0.6] [0.9,0.1][30m,6h][30m,12h][30m,24h]Wikipedia 3[1d,4d][1d,7d][1d,14d][1,0.8][1,0.6] [0.9,0.1]Table 7: Tested hyper-parameter values for the MG-TSD Model. The reported results in the paperare based on a parameter search within these choices.D A PPENDIX : M ETRICSMore details about the metrics we adopt can be found in Gluonts documentation (Alexandrov et al.,2020a). We briefly summarize them as below:CRPSsum : From de B ´ezenac et al. (2020), CRPS is a univariate strictly proper scoring rule whichmeasures the compatibility of a cumulative distribution function F with an observation x ∈ R asCRPS(F, x) =ZR(F(y) − 1(x ≤ y))2dywhere I{x ≤ y} is the indicator function, which is 1 ifx ≤ y and 0 otherwise. The CRPS attains theminimum value when the predictive distribution F same as the data distribution. CRPS sum extendsCRPS to multivariate time series with a simple modification.CRPSsum = Et[CRPS(F−1sum,Xixit)],17Published as a conference paper at ICLR 2024where F−1sum is calculated by summing samples across dimensions and then sorted to get quantiles. Asmaller CRPSsum indicates better performance.NMAE: NMAE is a normalized version of the Mean Absolute Error (MAE) that takes into consid-eration the scale of the target values. The formula for NMAE is as follows:NMAE = mean(|( ˆY − Y )|)mean(|Y |)Similarly, in this formula, ˆY represents the predicted time series, and Y represents the true targettime series. NMAE calculates the average absolute difference between predictions and true values,normalized by the mean absolute magnitude of the target values. A smaller NMAE implies moreaccurate predictions.NRMSE: NRMSE is a normalized adaptation of the Root Mean Squared Error (RMSE) that factorsin the scale of the target values. The formula for NRMSE is as follows:NRMSE =smean(( ˆY − Y )2)mean(|Y |)Here, ˆY represents the predicted time series, and Y represents the true target time series. NRMSEmeasures the average squared difference between predictions and true values, normalized by themean absolute magnitude of the target values. A smaller NRMSE indicates more accurate predic-tions.18Published as a conference paper at ICLR 2024E A PPENDIX : M ORE ILLUSTRATIVE PLOTS00:00 00:00 00:0006:0012:0018:00 06:0012:0018:00050100150200250Dim 000:00 00:00 00:0006:0012:0018:00 06:0012:0018:00050100150Dim 100:00 00:00 00:0006:0012:0018:00 06:0012:0018:000255075100125Dim 200:00 00:00 00:0006:0012:0018:00 06:0012:0018:00020406080Dim 300:00 00:00 00:0006:0012:0018:00 06:0012:0018:00050100150200Dim 400:00 00:00 00:0006:0012:0018:00 06:0012:0018:000100200300400Dim 500:00 00:00 00:0006:0012:0018:00 06:0012:0018:000255075100125Dim 600:00 00:00 00:0006:0012:0018:00 06:0012:0018:00020406080100Dim 700:00 00:00 00:0006:0012:0018:00 06:0012:0018:000204060Dim 800:00 00:00 00:0006:0012:0018:00 06:0012:0018:00020406080Dim 900:00 00:00 00:0006:0012:0018:00 06:0012:0018:000255075100125Dim 1000:00 00:00 00:0006:0012:0018:00 06:0012:0018:00020406080Dim 1100:00 00:00 00:0006:0012:0018:00 06:0012:0018:000204060Dim 1200:00 00:00 00:0006:0012:0018:00 06:0012:0018:000204060Dim 1300:00 00:00 00:0006:0012:0018:00 06:0012:0018:000255075100125Dim 1400:00 00:00 00:0006:0012:0018:00 06:0012:0018:000204060Dim 15ObservationsTimeGrad: median predictionMG-TSD: median prediction of 1hMG-TSD: median prediction of 4hTimeGrad: 50.0% prediction intervalMG-TSD: 50.0% prediction intervalFigure 8: MG-TSD and TimeGrad prediction intervals and test set ground-truth for Solar data ofsome illustrative dimensions of 370 dimensions from first rolling-window.19"
        }
    ]
}
