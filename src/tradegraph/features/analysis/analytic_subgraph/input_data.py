analytic_subgraph_input_data = {
    "new_method": """
Below is the outcome of step 3—a novel method that draws on the ideas behind the Base Method (D3PO) and the Q-Probe approach:\n\n─────────────────────────────  \nProposed Method: Q-Denoising Diffusion Probe (QD²P)\n\nOverview:\nQD²P rethinks how to incorporate human preferences into diffusion model fine-tuning by leveraging a lightweight, embedding‐space “probe” instead of updating the full model with DPO‐style losses. The idea is to freeze most of the diffusion model (as in Q-Probe) while training a simple linear “Q‐probe” on the intermediate latent embeddings generated during the denoising steps. This probe is tasked with estimating a quality score that reflects human feedback (or proxy signals) on the current latent state. Then, at each denoising step, the Q‐probe is used to reweight or select among candidate latent updates, guiding the denoising process toward outputs that better align with user preferences.\n\nKey Innovations:\n1. Dual-Stage Guidance:\n • Instead of directly fine-tuning the entire diffusion model with a preference loss (as in D3PO), QD²P first uses the pretrained diffusion process to generate multiple candidate latent updates per time step.\n • The simple linear Q‐probe (trained on human preference pairs) evaluates the quality of each candidate in the latent space.\n • The candidate with the highest “Q‐value” is selected for the next denoising step, thus “steering” the denoising trajectory according to human-aligned criteria.\n\n2. Light-weight and Efficient:\n • By freezing the bulk of the diffusion model and only training a low-parameter Q‐probe, the method dramatically reduces computational and memory overhead.\n • The reweighting mechanism avoids the need for a separately trained complex reward model and eliminates the heavy GPU memory requirements of directly applying DPO-style losses over many denoising steps.\n\n3. Embedded Feedback Loop:\n • The Q‐probe is updated using either direct human preference data or automated proxies, similar to D3PO’s reliance on human feedback but with a more interpretable, low-cost signal.\n • Over time, this simple linear mapping “learns” to score latent embeddings in a way that implicitly reflects an optimal reward function, enabling the method to simulate the benefits of direct preference optimization with far less overhead.\n\n4. Bridging Sampling and Optimization:\n • During inference or iterative generation, the method can generate k candidate latent proposals at each denoising step.\n • A softmax over the learned Q‐values governs selection, effectively performing a KL-constrained reweighting analogous to the Q-Probe’s treatment of language models.\n • Optionally, a subsequent distillation step can “absorb” the Q‐probe’s corrections back into a lightweight adaptation of the diffusion model (through methods like LoRA), further reducing inference costs.\n\nBenefits over the Base Method:\n • Mitigates reliance on heavy computational resources by bypassing full gradient updates on the diffusion model.\n • Reduces the strict need for extensive human feedback by embedding feedback directly at the latent level, where a simple Q‐probe is easier to train and update.\n • Offers a transparent, modular framework in which the quality assessment (via the probe) can be improved or calibrated independently of the main generation process.\n • Provides a mechanism for adaptive, stepwise reward correction without full reward model training, potentially enhancing stability and generalization.\n\nIn summary, Q-Denoising Diffusion Probe (QD²P) is a method that uses a lightweight, linear Q‐probe to guide the much costlier diffusion denoising process. It blends the direct human feedback optimization of D3PO with the efficient, embedding-space reweighting of Q-Probe, offering a novel fine-tuning strategy that is both resource-efficient and robust in aligning generated outputs with human preferences.
""",
    "verification_policy": """
Below is an experimental plan with three concrete experiments that can be implemented in Python (using libraries such as PyTorch, NumPy, and standard logging/visualization tools) to demonstrate the superiority of the Q-Denoising Diffusion Probe (QD²P) over a baseline diffusion method (e.g., D3PO):\n\n─────────────────────────────  \n1. Controlled Quality Evaluation on Synthetic Tasks\n\n• Objective:\n – Compare generation quality and user-aligned output preference between the baseline method and QD²P.\n – Use proxy human-feedback metrics (for example, similarity to a “desired” latent profile or a learned scoring function) to evaluate performance.\n\n• Experimental Outline:\n – Implement a simplified diffusion model pipeline that runs both the baseline (full-gradient updates using DPO-style losses) and QD²P (freezing the diffusion model and training a simple linear Q-probe).\n – Generate outputs from both methods over a series of denoising steps.\n – At each denoising step, for QD²P, generate k candidate latent proposals and select the one with the best Q-value via softmax reweighting.\n – Measure output quality with quantitative metrics such as Mean Squared Error (if a ground truth exists) or content-based metrics (e.g., cosine similarity in embedding space to an “ideal” latent vector).\n – Visualize performance differences over multiple runs and noise levels.\n\n• Python Implementation Aspects:\n – Code candidate generation using torch.tensor operations.\n – Use a linear layer for the Q-probe, trainable on proxy preference pairs.\n – Log and plot quality metric trends and computational time.\n\n─────────────────────────────  \n2. Ablation Study on the Candidate Reweighting Mechanism\n\n• Objective:\n – Isolate and understand the contribution of the latent candidate reweighting procedure used in QD²P.\n – Vary the number of candidate latent proposals (k) and assess how the softmax temperature or weighting biases affect the quality and stability of the denoising process.\n\n• Experimental Outline:\n – Modify the code to loop over experiments with varying k (e.g., 2, 5, 10 candidates per denoising step).\n – Optionally, incorporate a temperature parameter in the softmax to see its impact on selection diversity.\n – Use the same underlying diffusion model while only changing the reweighting mechanism.\n – Compare quantitative performance (quality scores, convergence speed) and qualitative differences in generated outputs.\n\n• Python Implementation Aspects:\n – Implement candidate proposal generation as a for-loop or batched operation in PyTorch.\n – Tweak the softmax function with different temperature values.\n – Use logs to record selected candidate indices and Q-values at different steps for later analysis.\n\n─────────────────────────────  \n3. Efficiency and Distillation: Reducing Inference Overhead\n\n• Objective:\n – Demonstrate that QD²P offers a resource-efficient alternative by comparing inference times and memory consumption.\n – Explore the optional distillation step—where corrections learned by the Q-probe are “absorbed” back into a lightweight adaptation (e.g., via LoRA or another low-rank adapter) of the diffusion model.\n\n• Experimental Outline:\n – Measure baseline inference times and memory usage for the full diffusion pipeline versus QD²P.\n – After training the Q-probe, run an experiment where you “distill” the probe’s corrections into a small adaptation module.\n – Compare the quality and inference cost (time and GPU memory) between:\n  • The original QD²P method (with candidate evaluation at every denoising step), and\n  • The distilled / adapted model using LoRA-like techniques.\n – Validate that distillation maintains or slightly degrades quality while significantly reducing computational overhead.\n\n• Python Implementation Aspects:\n – Use Python timing libraries (e.g., time or torch.cuda.synchronize) and memory profiling tools (e.g., torch.cuda.memory_allocated) to log resource usage.\n – Implement a small LoRA-like adapter, reusing the Q-probe outputs to update only a few extra parameters.\n – Plot inference times and memory consumption alongside performance metrics.\n\n─────────────────────────────  \nSummary\n\nThese experiments are designed to be realistic and executable in Python:\n • They combine quantitative metric tracking, resource profiling, and controlled variations on key components (candidate generation and reweighting) within a diffusion generation pipeline.\n • The experiments can be run on simulated datasets or with established diffusion model benchmarks, ensuring reproducible and interpretable results.\n • The insights gained from these experiments (i.e., quality improvement, efficiency gains, and robustness of the reweighting mechanism) will demonstrate the practical superiority of QD²P.\n\nThis plan provides a modular and interpretable approach to experimentally validating the proposed method’s contributions while being fully implementable with standard Python and deep learning libraries.",
""",
    "experiment_code": """
#!/usr/bin/env python3\n\"\"\"\nDetailed experimental code comparing a baseline full-gradient diffusion method with\nthe proposed Q-Denoising Diffusion Probe (QD²P) over three experiments:\n1. Controlled Quality Evaluation on Synthetic Tasks,\n2. Ablation Study on the Candidate Reweighting Mechanism, and\n3. Efficiency and Distillation: Reducing Inference Overhead.\n\nRequired Python libraries:\n    • torch\n    • torch.nn\n    • torch.nn.functional\n    • torch.optim\n    • matplotlib\n    • time\n    • logging\n    • (Optionally) seaborn for improved plot aesthetics\n\nAll plots are saved as .pdf files following the naming convention:\n    <figure_topic>[_<condition>][_pairN].pdf\n\nA test function (“test_all_experiments”) is included to verify that the code executes\nquickly; it uses reduced iterations to ensure an immediate finish.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport time\nimport logging\n\n# Set logging format\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# ----------------------------\n# Model Components and Utilities\n# ----------------------------\n\nclass SimpleDiffusion(nn.Module):\n    \"\"\"\n    A toy diffusion module with a single linear transformation plus additive noise.\n    \"\"\"\n    def __init__(self, latent_dim):\n        super(SimpleDiffusion, self).__init__()\n        self.linear = nn.Linear(latent_dim, latent_dim)\n\n    def forward(self, x, noise_level):\n        noise = noise_level * torch.randn_like(x)\n        return self.linear(x) + noise\n\nclass QProbe(nn.Module):\n    \"\"\"\n    A Q-probe module for scoring candidates.\n    \"\"\"\n    def __init__(self, latent_dim):\n        super(QProbe, self).__init__()\n        self.linear = nn.Linear(latent_dim, 1)\n\n    def forward(self, x):\n        return self.linear(x)\n\ndef quality_metric(output, target):\n    \"\"\"\n    Proxy quality metric: Mean cosine similarity between output and target.\n    \"\"\"\n    output_norm = F.normalize(output, dim=1)\n    target_norm = F.normalize(target, dim=1)\n    similarity = (output_norm * target_norm).sum(dim=1).mean()\n    return similarity\n\n# ----------------------------\n# Experiment 1: Controlled Quality Evaluation on Synthetic Tasks\n# ----------------------------\n\ndef controlled_quality_experiment(latent_dim=16, steps=10, candidate_count=5):\n    \"\"\"\n    Compare baseline diffusion with QD²P using synthetic latent vectors.\n    Two quality metrics are computed per denoising step.\n    The quality metric (cosine similarity to an ideal latent vector) is plotted.\n    \"\"\"\n    logging.info(\"Starting Experiment 1: Controlled Quality Evaluation on Synthetic Tasks.\")\n    \n    # Instantiate models\n    diffusion_model = SimpleDiffusion(latent_dim)\n    q_probe = QProbe(latent_dim)\n    optimizer = optim.Adam(q_probe.parameters(), lr=0.01)\n\n    # Synthetic ideal latent vector (all ones)\n    ideal = torch.ones((1, latent_dim))\n    \n    batch_size = 32\n    latent = torch.randn(batch_size, latent_dim)\n\n    quality_scores_baseline = []\n    quality_scores_qd2p = []\n\n    for step in range(steps):\n        noise_level = 1.0 / (step + 1)\n        logging.info(\"Step %d, noise_level = %.4f\", step, noise_level)\n        \n        # Baseline: one diffusion step\n        latent_baseline = diffusion_model(latent, noise_level)\n        score_baseline = quality_metric(latent_baseline, ideal.expand_as(latent_baseline))\n        quality_scores_baseline.append(score_baseline.item())\n\n        # QD²P: Generate k candidates and compute Q-values using Q-probe\n        candidates = []\n        q_values = []\n        for k in range(candidate_count):\n            candidate = diffusion_model(latent, noise_level)\n            candidates.append(candidate)\n            q_val = q_probe(candidate)  # shape: [batch, 1]\n            q_values.append(q_val)\n        candidates_tensor = torch.stack(candidates)  # shape: [k, batch, latent_dim]\n        q_values_tensor = torch.stack(q_values).squeeze(-1)  # shape: [k, batch]\n\n        # Reweight candidate proposals with softmax (default temperature=1)\n        weights = F.softmax(q_values_tensor, dim=0)  # shape: [k, batch]\n        weighted_candidate = (weights.unsqueeze(-1) * candidates_tensor).sum(dim=0)\n\n        score_qd2p = quality_metric(weighted_candidate, ideal.expand_as(weighted_candidate))\n        quality_scores_qd2p.append(score_qd2p.item())\n        \n        # A dummy loss update for Q-probe (using MSE loss)\n        loss = F.mse_loss(q_values_tensor.mean(dim=0), torch.ones_like(q_values_tensor.mean(dim=0)))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Update latent for next step (using baseline output for simplicity)\n        latent = latent_baseline.detach()\n\n    # Plot quality metric curves (save as PDF)\n    plt.figure(figsize=(8, 4))\n    plt.plot(range(steps), quality_scores_baseline, label=\"Baseline Diffusion\", marker='o')\n    plt.plot(range(steps), quality_scores_qd2p, label=\"QD²P\", marker='s')\n    plt.xlabel(\"Denoising Step\")\n    plt.ylabel(\"Quality Metric (Cosine Similarity)\")\n    plt.title(\"Quality Metrics over Denoising Steps\")\n    plt.legend()\n    filename = \"quality_metric_synthetic.pdf\"\n    plt.savefig(filename)\n    plt.close()\n    logging.info(\"Experiment 1 plot saved as %s\", filename)\n    \n    print(\"Experiment 1 Results:\")\n    print(\"Baseline Quality Scores:\", quality_scores_baseline)\n    print(\"QD²P Quality Scores:\", quality_scores_qd2p)\n\n    return quality_scores_baseline, quality_scores_qd2p\n\n# ----------------------------\n# Experiment 2: Ablation Study on the Candidate Reweighting Mechanism\n# ----------------------------\n\ndef ablation_candidate_reweighting(latent_dim=16, steps=10, candidate_counts=[2, 5, 10], temperatures=[0.5, 1.0, 2.0]):\n    \"\"\"\n    Varies the number of candidate proposals 'k' and the softmax temperature 'T'\n    to study their effect on the quality metric.\n    A plot comparing different configurations is saved.\n    \"\"\"\n    logging.info(\"Starting Experiment 2: Ablation Study on Candidate Reweighting Mechanism.\")\n    \n    diffusion_model = SimpleDiffusion(latent_dim)\n    q_probe = QProbe(latent_dim)\n\n    results = {}\n    \n    for k in candidate_counts:\n        for temp in temperatures:\n            config_key = f\"k={k}_T={temp}\"\n            quality_scores = []\n            latent = torch.randn(32, latent_dim)\n            for step in range(steps):\n                noise_level = 1.0 / (step + 1)\n                candidates = []\n                q_values = []\n                for _ in range(k):\n                    candidate = diffusion_model(latent, noise_level)\n                    candidates.append(candidate)\n                    q_values.append(q_probe(candidate))\n                candidates_tensor = torch.stack(candidates)    # [k, batch, latent_dim]\n                q_values_tensor = torch.stack(q_values).squeeze(-1)  # [k, batch]\n                # Use temperature T in softmax reweighting\n                weights = F.softmax(q_values_tensor / temp, dim=0)\n                weighted_candidate = (weights.unsqueeze(-1) * candidates_tensor).sum(dim=0)\n                # Use an ideal latent vector of ones for the quality metric\n                quality = quality_metric(weighted_candidate, torch.ones((1, latent_dim)).expand_as(weighted_candidate))\n                quality_scores.append(quality.item())\n                latent = weighted_candidate.detach()  # update for next step\n            results[config_key] = quality_scores\n            logging.info(\"Config %s completed.\", config_key)\n    \n    # Plotting all configurations on one figure (save as PDF)\n    plt.figure(figsize=(10, 6))\n    for key, scores in results.items():\n        plt.plot(range(steps), scores, label=key, marker='o')\n    plt.xlabel(\"Denoising Step\")\n    plt.ylabel(\"Quality Metric (Cosine Similarity)\")\n    plt.title(\"Ablation Study: Candidate Count and Temperature Impact\")\n    plt.legend()\n    filename = \"ablation_candidate_reweighting.pdf\"\n    plt.savefig(filename)\n    plt.close()\n    logging.info(\"Experiment 2 plot saved as %s\", filename)\n    \n    print(\"Experiment 2 Results (Quality Metric per Configuration):\")\n    for config, scores in results.items():\n        print(f\"  {config}: {scores}\")\n    \n    return results\n\n# ----------------------------\n# Experiment 3: Efficiency and Distillation (Reducing Inference Overhead)\n# ----------------------------\n\ndef lora_adapter_module(input_tensor, adapter_params):\n    \"\"\"\n    A simple LoRA-like lightweight adapter: performs a low-rank update\n    by adding a linear correction (here implemented as a matrix multiplication).\n    \"\"\"\n    return input_tensor + input_tensor @ adapter_params\n\ndef efficiency_and_distillation(latent_dim=16, steps=10, candidate_count=5):\n    \"\"\"\n    Compares inference time and resource overhead between full QD²P (which uses candidate\n    reweighting in every step) and a distilled version that uses a learned adapter module\n    to mimic the Q-probe's corrections.\n    \"\"\"\n    logging.info(\"Starting Experiment 3: Efficiency and Distillation.\")\n    \n    diffusion_model = SimpleDiffusion(latent_dim)\n    q_probe = QProbe(latent_dim)\n    # (In practice, Q-probe would be pre-trained. Here we simulate distinction.)\n    \n    # Measure full QD²P inference runtime\n    latent = torch.randn(32, latent_dim)\n    start_time = time.time()\n    for step in range(steps):\n        noise_level = 1.0 / (step + 1)\n        candidates = []\n        q_values = []\n        for _ in range(candidate_count):\n            candidate = diffusion_model(latent, noise_level)\n            candidates.append(candidate)\n            q_values.append(q_probe(candidate))\n        candidates_tensor = torch.stack(candidates)  # [k, batch, latent_dim]\n        q_values_tensor = torch.stack(q_values).squeeze(-1)  # [k, batch]\n        weights = F.softmax(q_values_tensor, dim=0)\n        weighted_candidate = (weights.unsqueeze(-1) * candidates_tensor).sum(dim=0)\n        latent = weighted_candidate.detach()\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    full_qd2p_time = time.time() - start_time\n\n    # Distillation: use diffusion model followed by a lightweight adapter (simulate LoRA)\n    adapter_params = torch.randn(latent_dim, latent_dim) * 0.01  # lightweight parameters\n    latent = torch.randn(32, latent_dim)\n    distilled_outputs = []\n    start_time = time.time()\n    for step in range(steps):\n        noise_level = 1.0 / (step + 1)\n        latent_diffused = diffusion_model(latent, noise_level)\n        # Apply adapter instead of candidate reweighting\n        latent = lora_adapter_module(latent_diffused, adapter_params)\n        distilled_outputs.append(latent)\n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    distilled_time = time.time() - start_time\n\n    # Quality metrics (comparing final latent to an ideal latent vector of ones)\n    ideal = torch.ones((32, latent_dim))\n    quality_full = quality_metric(latent_diffused, ideal)\n    quality_distilled = quality_metric(latent, ideal)\n    \n    print(\"Full QD²P Inference Time: {:.4f} s, Quality: {:.4f}\".format(full_qd2p_time, quality_full))\n    print(\"Distilled Inference Time: {:.4f} s, Quality: {:.4f}\".format(distilled_time, quality_distilled))\n    \n    if torch.cuda.is_available():\n        mem_usage = torch.cuda.memory_allocated() / (1024 * 1024)\n        print(\"Current GPU Memory Usage: {:.2f} MB\".format(mem_usage))\n    else:\n        print(\"CUDA is not available; skipping GPU memory profiling.\")\n\n    # Create a bar plot comparing inference times (save as PDF)\n    methods = ['Full QD²P', 'Distilled']\n    times = [full_qd2p_time, distilled_time]\n    plt.figure(figsize=(6, 4))\n    plt.bar(methods, times, color=[\"blue\", \"green\"])\n    plt.ylabel(\"Inference Time (s)\")\n    plt.title(\"Inference Time Comparison\")\n    filename = \"inference_latency_comparison.pdf\"\n    plt.savefig(filename)\n    plt.close()\n    logging.info(\"Experiment 3 inference latency plot saved as %s\", filename)\n    \n    return full_qd2p_time, distilled_time, quality_full.item(), quality_distilled.item()\n\n# ----------------------------\n# Test Functions\n# ----------------------------\n\ndef test_controlled_quality_experiment():\n    \"\"\"\n    Run Experiment 1 with reduced steps for a quick test.\n    \"\"\"\n    print(\"\\nRunning quick test for Experiment 1 (Controlled Quality Evaluation)...\")\n    controlled_quality_experiment(latent_dim=8, steps=3, candidate_count=3)\n    print(\"Test for Experiment 1 completed.\\n\")\n\ndef test_ablation_candidate_reweighting():\n    \"\"\"\n    Run Experiment 2 with reduced steps for a quick test.\n    \"\"\"\n    print(\"\\nRunning quick test for Experiment 2 (Ablation Candidate Reweighting)...\")\n    ablation_candidate_reweighting(latent_dim=8, steps=3, candidate_counts=[2, 3], temperatures=[0.5, 1.0])\n    print(\"Test for Experiment 2 completed.\\n\")\n\ndef test_efficiency_and_distillation():\n    \"\"\"\n    Run Experiment 3 with reduced steps for a quick test.\n    \"\"\"\n    print(\"\\nRunning quick test for Experiment 3 (Efficiency and Distillation)...\")\n    efficiency_and_distillation(latent_dim=8, steps=3, candidate_count=2)\n    print(\"Test for Experiment 3 completed.\\n\")\n\ndef test_all_experiments():\n    \"\"\"\n    Runs all test functions to verify that the experimental code executes correctly.\n    \"\"\"\n    start_overall = time.time()\n    test_controlled_quality_experiment()\n    test_ablation_candidate_reweighting()\n    test_efficiency_and_distillation()\n    overall_time = time.time() - start_overall\n    print(\"All tests completed in {:.4f} seconds.\".format(overall_time))\n\n# ----------------------------\n# Main Execution\n# ----------------------------\n\nif __name__ == \"__main__\":\n    # For demonstration, run the quick tests. In practice, you may call the full experiments.\n    test_all_experiments()\n\n    # Optionally, uncomment the lines below to run the full experiments:\n    # controlled_quality_experiment(latent_dim=16, steps=10, candidate_count=5)\n    # ablation_candidate_reweighting(latent_dim=16, steps=10, candidate_counts=[2, 5, 10], temperatures=[0.5, 1.0, 2.0])\n    # efficiency_and_distillation(latent_dim=16, steps=10, candidate_count=5)\n    \n    logging.info(\"All experiments completed.\")\n    \n# End of the experimental script.\n
""",
    "output_text_data": """
\nSystem Information:\n- PyTorch version: 2.6.0+cu124\n- CUDA available: True\n- CUDA device: Tesla T4\n- CUDA version: 12.4\n- GPU memory: 16.71 GB\n- Device being used: cuda\n\n\n==================================================\nQD²P: Q-Denoising Diffusion Probe Experiment\n==================================================\n\nExperiment Configuration:\n- Latent Dimension: 16\n- Batch Size: 32\n- Diffusion Steps: 10\n- Candidate Count: 5\n- Random Seed: 42\n- Device: cuda\n\nStarting data generation...\nSynthetic data created with shape: torch.Size([32, 16])\n\n========================\n===  Model Training  ===\n========================\n\nTraining SimpleDiffusion model and QProbe...\n- Optimizer: Adam with learning rate 0.01\n- Loss function: MSE between Q-values and quality metrics\n- Training for 10 diffusion steps\nTraining completed in 1.55 seconds\n\n=====================================================\n===  Experiment 1: Controlled Quality Evaluation  ===\n=====================================================\n\nRunning controlled quality evaluation on synthetic tasks...\n- Comparing baseline diffusion vs. QD²P with 5 candidates\n- Measuring quality improvement over 10 denoising steps\n- Using cosine similarity as quality metric\nExperiment 1 Results:\nBaseline Quality Scores: [-0.06120230257511139, -0.05738872289657593, -0.006666048429906368, -0.02784591168165207, 0.02714875154197216, 0.0830594003200531, 0.06916013360023499, 0.0015294961631298065, -0.10152167081832886, -0.03565112501382828]\nQD²P Quality Scores: [-0.08630995452404022, -0.04035212844610214, 0.017857590690255165, -0.027536433190107346, 0.024290243163704872, 0.07558532059192657, 0.07184366136789322, -0.0012599080801010132, -0.10069799423217773, -0.03422578424215317]\n\nExperiment 1 Results:\n- Initial baseline quality: -0.0612\n- Final baseline quality: -0.0357\n- Initial QD²P quality: -0.0863\n- Final QD²P quality: -0.0342\n- Quality improvement: 0.0014\n- Figure saved to: logs/quality_metric_synthetic.pdf\n\n===============================================================\n===  Experiment 2: Ablation Study on Candidate Reweighting  ===\n===============================================================\n\nRunning ablation study on candidate reweighting mechanism...\n- Testing 3 different candidate counts: [2, 5, 10]\n- Testing 3 different temperature values: [0.5, 1.0, 2.0]\n- Evaluating 9 total configurations\nExperiment 2 Results (Quality Metric per Configuration):\n  k=2_T=0.5: [-0.07621617615222931, -0.11515659093856812, 0.068050816655159, -0.04124131053686142, 0.009582923725247383, 0.06185685843229294, -0.04168466851115227, 0.030201150104403496, -0.030731257051229477, -0.02392379194498062]\n  k=2_T=1.0: [-0.029074421152472496, -0.07147807627916336, 0.0112045519053936, -0.0007719621062278748, 0.05557234585285187, 0.021243907511234283, -0.005639784038066864, 0.02583164907991886, -0.095853790640831, -0.05546446144580841]\n  k=2_T=2.0: [-0.02749866247177124, 0.0010265065357089043, -0.01007329672574997, -0.09452344477176666, 0.051887258887290955, 0.04106584191322327, -0.016919247806072235, -0.018448706716299057, -0.02950361929833889, 0.06914735585451126]\n  k=5_T=0.5: [-0.010603133589029312, -0.08950018882751465, 0.06197163835167885, -0.02372167631983757, -0.024924391880631447, 0.07102400064468384, 0.01004222221672535, 0.03290200233459473, -0.019451364874839783, 0.015017314814031124]\n  k=5_T=1.0: [-0.028052963316440582, -0.06307726353406906, 0.024948153644800186, -0.016355253756046295, 0.03979872167110443, 0.03191566839814186, 0.021097155287861824, 0.03176296129822731, -0.06864077597856522, 0.0028780149295926094]\n  k=5_T=2.0: [-0.053990788757801056, -0.058372482657432556, -0.005517570301890373, -0.044137969613075256, -0.0165758915245533, 0.02614065259695053, 0.023695722222328186, 0.004935026168823242, -0.053746871650218964, 0.025129273533821106]\n  k=10_T=0.5: [-0.03623402118682861, -0.07199186086654663, 0.023637376725673676, -0.04925376549363136, 0.004995770752429962, 0.05485934764146805, 0.0605277456343174, 0.013007726520299911, -0.07442155480384827, 0.021338585764169693]\n  k=10_T=1.0: [-0.05882226675748825, -0.05293327569961548, 0.004807446151971817, -0.04527188464999199, -0.020269129425287247, 0.04895075410604477, 0.062236394733190536, 0.028785280883312225, -0.06107566878199577, -0.011211996898055077]\n  k=10_T=2.0: [-0.05475359410047531, -0.06344838440418243, 0.0013384046033024788, -0.04972434043884277, 0.002585785463452339, 0.03407450392842293, 0.0548241026699543, 0.02170976810157299, -0.08484633266925812, -0.019152069464325905]\n\nExperiment 2 Results:\n- Best configuration: k=2_T=2.0 with final quality score: 0.0691\n- k=2_T=0.5: initial=-0.0762, final=-0.0239\n- k=2_T=1.0: initial=-0.0291, final=-0.0555\n- k=2_T=2.0: initial=-0.0275, final=0.0691\n- k=5_T=0.5: initial=-0.0106, final=0.0150\n- k=5_T=1.0: initial=-0.0281, final=0.0029\n- k=5_T=2.0: initial=-0.0540, final=0.0251\n- k=10_T=0.5: initial=-0.0362, final=0.0213\n- k=10_T=1.0: initial=-0.0588, final=-0.0112\n- k=10_T=2.0: initial=-0.0548, final=-0.0192\n- Figure saved to: logs/ablation_candidate_reweighting.pdf\n\n===================================================\n===  Experiment 3: Efficiency and Distillation  ===\n===================================================\n\nRunning efficiency and distillation experiment...\n- Comparing full QD²P vs. distilled version\n- Measuring inference time and quality metrics\n- Using lightweight adapter for distillation\nFull QD²P Inference Time: 0.0085 s, Quality: -0.0174\nDistilled Inference Time: 0.0014 s, Quality: -0.0174\nCurrent GPU Memory Usage: 16.32 MB\n\nExperiment 3 Results:\n- Full QD²P Inference Time: 0.0085 seconds\n- Distilled Inference Time: 0.0014 seconds\n- Speedup from distillation: 6.30x\n- Full QD²P Quality: -0.0174\n- Distilled Quality: -0.0174\n- Quality difference: 0.0000\n- Latency figure saved to: logs/inference_latency_comparison.pdf\n- Quality figure saved to: logs/quality_comparison.pdf\n\n============================\n===  Summary of Results  ===\n============================\n\nQD²P Experiment Summary:\n\nExperiment 1 - Controlled Quality Evaluation:\n- Quality improvement: 0.0014\n- Baseline final quality: -0.0357\n- QD²P final quality: -0.0342\n\nExperiment 2 - Ablation Study:\n- Best configuration: k=2_T=2.0\n- Best configuration score: 0.0691\n- Total configurations tested: 9\n\nExperiment 3 - Efficiency and Distillation:\n- Speedup from distillation: 6.30x\n- Quality retention: 100.0%\n\nGenerated Figures:\n- Experiment 1 (Quality Evaluation): logs/quality_metric_synthetic.pdf\n- Experiment 2 (Ablation Study): logs/ablation_candidate_reweighting.pdf\n- Experiment 3 (Latency Comparison): logs/inference_latency_comparison.pdf\n- Experiment 3 (Quality Comparison): logs/quality_comparison.pdf\n\nTotal execution time: 2.69 seconds\n\nExperiment completed successfully!\n
""",
}
